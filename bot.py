import feedparser
import tweepy
import requests
import os
import sys
import time
import textwrap
import re
from difflib import SequenceMatcher
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont

# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ==========================================
def get_clean_env(name):
    val = os.environ.get(name)
    if val is None: return None
    return val.strip().replace('\n', '').replace('\r', '').replace(' ', '')

GEMINI_API_KEY = get_clean_env("GEMINI_API_KEY")
CONSUMER_KEY = get_clean_env("CONSUMER_KEY")
CONSUMER_SECRET = get_clean_env("CONSUMER_SECRET")
ACCESS_TOKEN = get_clean_env("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = get_clean_env("ACCESS_TOKEN_SECRET")

# ==========================================
# 2. íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
# ==========================================
client = None
api = None
try:
    client = tweepy.Client(
        consumer_key=CONSUMER_KEY,
        consumer_secret=CONSUMER_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_TOKEN_SECRET
    )
    auth = tweepy.OAuth1UserHandler(
        CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET
    )
    api = tweepy.API(auth)
except Exception as e:
    print(f"âš ï¸ íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")

# ==========================================
# 3. ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
# ==========================================
RSS_SOURCES = [
    ("ë¯¸êµ­ì£¼ì‹(íˆ¬ì)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069", "last_link_us_investing.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸ˆìœµ)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664", "last_link_us_finance.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸°ìˆ )", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910", "last_link_us_tech.txt", "CNBC"),
    ("í•œêµ­ì£¼ì‹(í•œê²½)", "https://www.hankyung.com/feed/finance", "last_link_kr.txt", "í•œêµ­ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(Yahoo)", "https://finance.yahoo.com/news/rssindex", "last_link_yahoo.txt", "Yahoo Finance"),
    ("ë¯¸êµ­ì£¼ì‹(Tech)", "https://techcrunch.com/feed/", "last_link_techcrunch.txt", "TechCrunch"),
    ("í•œêµ­ì£¼ì‹(ë§¤ê²½)", "https://www.mk.co.kr/rss/50200011/", "last_link_mk.txt", "ë§¤ì¼ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Opinion)", "https://feeds.content.dowjones.io/public/rss/RSSOpinion", "last_link_wsj_op.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Market)", "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain", "last_link_wsj_mkt.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Economy)", "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed", "last_link_wsj_eco.txt", "WSJ"),
    ("ì†ë³´(í…”ë ˆê·¸ë¨)", "https://rsshub.app/telegram/channel/bornlupin", "last_link_bornlupin.txt", "Telegram")
]

# â˜… [ìˆ˜ì •] ê¸°ì–µí•  íˆìŠ¤í† ë¦¬ ê°œìˆ˜ (300ê°œë¡œ ìƒí–¥)
MAX_HISTORY = 300
GLOBAL_TITLE_FILE = "processed_global_titles.txt"

# ==========================================
# 4. ì¹´ë“œë‰´ìŠ¤ ìƒì„± í•¨ìˆ˜
# ==========================================
def create_info_image(text_lines, source_name):
    try:
        width, height = 1200, 675 
        background_color = (18, 18, 18)
        text_color = (235, 235, 235)
        title_color = (255, 255, 255)
        accent_color = (0, 190, 255)
        
        image = Image.new('RGB', (width, height), background_color)
        draw = ImageDraw.Draw(image)
        
        font_path = "font.ttf"
        try:
            title_font = ImageFont.truetype(font_path, 54) 
            body_font = ImageFont.truetype(font_path, 32)
            source_font = ImageFont.truetype(font_path, 24)
        except:
            print("âš ï¸ í°íŠ¸ íŒŒì¼ ì˜¤ë¥˜")
            return None

        margin_x = 80       
        header_y = 45
        
        if source_name:
            header_text = f"Market Radar | {source_name}"
        else:
            header_text = "Market Radar"
        draw.text((margin_x, header_y), header_text, font=source_font, fill=accent_color)
        draw.text((margin_x, header_y + 30), "@marketradar0", font=source_font, fill=text_color)

        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST)
        date_str = f"{now.year % 100}ë…„ {now.month}ì›” {now.day}ì¼"

        try:
            date_width = draw.textlength(date_str, font=source_font)
        except AttributeError:
            date_width = source_font.getsize(date_str)[0]
            
        draw.text((width - margin_x - date_width, header_y), date_str, font=source_font, fill=text_color)

        current_y = 140     
        for i, line in enumerate(text_lines):
            line = line.strip().replace("**", "").replace("##", "")
            if not line: continue

            if i == 0: 
                wrapped_lines = textwrap.wrap(line, width=18)
                for wl in wrapped_lines:
                    draw.text((margin_x, current_y), wl, font=title_font, fill=title_color)
                    current_y += 70
                current_y += 25
                draw.line([(margin_x, current_y), (width-margin_x, current_y)], fill=(80, 80, 80), width=2)
                current_y += 45
            else: 
                bullet_size = 10
                bullet_y = current_y + 12
                draw.rectangle(
                    [margin_x - 25, bullet_y, margin_x - 25 + bullet_size, bullet_y + bullet_size],
                    fill=accent_color
                )
                wrapped_lines = textwrap.wrap(line, width=35)
                for wl in wrapped_lines:
                    draw.text((margin_x, current_y), wl, font=body_font, fill=text_color)
                    current_y += 42
                current_y += 10
            
            if current_y > height - 50: break 
        
        temp_filename = "temp_card_16_9.png"
        image.save(temp_filename)
        return temp_filename
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì—ëŸ¬: {e}")
        return None

# ==========================================
# 5. AI ëª¨ë¸ ì°¾ê¸°
# ==========================================
def get_working_model():
    print("ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ê²€ìƒ‰ ì¤‘...")
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GEMINI_API_KEY}"
    preferred_order = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro"]
    try:
        response = requests.get(url)
        if response.status_code == 200:
            models = [m['name'].replace('models/', '') for m in response.json().get('models', []) if 'generateContent' in m.get('supportedGenerationMethods', [])]
            for pref in preferred_order:
                for model in models:
                    if pref in model: 
                        print(f"âœ… ëª¨ë¸ ì°¾ìŒ: {model}")
                        return model
            if models: return models[0]
    except: pass
    return "gemini-1.5-flash"

# ==========================================
# 6. AI ìš”ì•½ í•¨ìˆ˜
# ==========================================
def summarize_news(target_model, title, link, content_text=""):
    prompt = f"""
    ë‰´ìŠ¤ ì œëª©: {title}
    ë‰´ìŠ¤ ë§í¬: {link}
    ë‰´ìŠ¤ ë‚´ìš©(Raw): {content_text}

    ìœ„ ë‚´ìš©ì„ ë¶„ì„í•´ì„œ íŠ¸ìœ„í„° ë³¸ë¬¸, ì¸í¬ê·¸ë˜í”½ í…ìŠ¤íŠ¸, ê·¸ë¦¬ê³  'ì›ì²œ ì†ŒìŠ¤ ì¶œì²˜'ë¥¼ ì°¾ì•„ì¤˜.

    [ì‘ì„± ê·œì¹™ 1: íŠ¸ìœ„í„° ë³¸ë¬¸]
    - êµ¬ë¶„ì: ---BODY--- ì•„ë˜ì— ì‘ì„±
    - í˜•ì‹: X í”„ë¦¬ë¯¸ì—„ìš© ì¥ë¬¸ ìƒì„¸ ìš”ì•½.
    - ìŠ¤íƒ€ì¼: í•œêµ­ì–´ ë²ˆì—­ í•„ìˆ˜. ëª…ì‚¬í˜• ì¢…ê²°ì´ë‚˜ ìŒìŠ´ì²´(~í•¨, ~ì„). ì¡´ëŒ“ë§ ê¸ˆì§€.
    - êµ¬ì„±: ì œëª©(ì´ëª¨ì§€+í•œê¸€), ìƒì„¸ ë‚´ìš©(âœ… ì²´í¬í¬ì¸íŠ¸), í•˜ë‹¨ í‹°ì»¤($)+í•´ì‹œíƒœê·¸(#)

    [ì‘ì„± ê·œì¹™ 2: ì¸í¬ê·¸ë˜í”½ ì´ë¯¸ì§€]
    - êµ¬ë¶„ì: ---IMAGE--- ì•„ë˜ì— ì‘ì„±
    - êµ¬ì„±:
      1. ì²« ì¤„: ê°•ë ¬í•œ í•œê¸€ ì œëª©. (ì´ëª¨ì§€ X). **ê¸°ì‚¬ í•µì‹¬ ìˆ˜ì¹˜(1400ì¡°, 2025ë…„ ë“±)ëŠ” ì œëª©ì— ë°˜ë“œì‹œ í¬í•¨.**
      2. ë‚˜ë¨¸ì§€: í•µì‹¬ ìš”ì•½ ë¬¸ì¥ ìµœëŒ€ 7ê°œ ì´ë‚´. **ë¬¸ì¥ ì‹œì‘ì— ì˜¤ëŠ” ìˆ«ì(ì—°ë„, ê¸ˆì•¡)ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ë§ ê²ƒ.**

    [ì‘ì„± ê·œì¹™ 3: ì›ì²œ ì†ŒìŠ¤ ì°¾ê¸°]
    - êµ¬ë¶„ì: ---SOURCE--- ì•„ë˜ì— ì‘ì„±
    - ê·œì¹™ A: ë§í¬ê°€ ìˆë‹¤ë©´ í•´ë‹¹ ì–¸ë¡ ì‚¬ ì´ë¦„(Bloomberg, WSJ ë“±).
    - ê·œì¹™ B: ë§í¬ê°€ ì—†ë‹¤ë©´ ë³¸ë¬¸ì—ì„œ 'Source:', 'ì¶œì²˜:', 'via' ë’¤ì— ë‚˜ì˜¤ëŠ” ê¸°ê´€ëª….
    - ê·œì¹™ C: ë§í¬ë„ ì—†ê³  í…ìŠ¤íŠ¸ ì–¸ê¸‰ë„ ì—†ìœ¼ë©´ 'Unknown'ì´ë¼ê³  ì ì–´. í…”ë ˆê·¸ë¨ ì±„ë„ëª…ì€ ì ì§€ ë§ˆ.

    [ê¸ˆì§€ì‚¬í•­]
    - ë§ˆí¬ë‹¤ìš´(**, ##) ì‚¬ìš© ê¸ˆì§€.
    """

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{target_model}:generateContent?key={GEMINI_API_KEY}"
    
    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "safetySettings": [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
    }
    headers = {'Content-Type': 'application/json'}

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                full_text = response.json()['candidates'][0]['content']['parts'][0]['text']
                if "---BODY---" in full_text and "---IMAGE---" in full_text:
                    parts = full_text.split("---IMAGE---")
                    body_raw = parts[0].replace("---BODY---", "").strip()
                    remaining = parts[1]
                    
                    if "---SOURCE---" in remaining:
                        img_parts = remaining.split("---SOURCE---")
                        image_raw = img_parts[0].strip()
                        source_raw = img_parts[1].strip()
                    else:
                        image_raw = remaining.strip()
                        source_raw = "Unknown" 

                    body_part = body_raw.replace("**", "").replace("##", "")
                    image_lines = []
                    for line in image_raw.split('\n'):
                        clean_line = line.strip().replace("**", "").replace("##", "")
                        clean_line = re.sub(r"^[\-\*\â€¢\Â·\âœ…\âœ”\â–ª\â–«\â–º]+\s*", "", clean_line)
                        clean_line = re.sub(r"^\d+\.\s+", "", clean_line)
                        if clean_line: image_lines.append(clean_line)
                    
                    source_name = source_raw.split('\n')[0].strip()
                    if "Unknown" in source_name or len(source_name) > 20: source_name = None 
                    
                    return body_part, image_lines, source_name
                else: return None, None, None
            elif response.status_code == 429:
                print(f"â³ API í•œë„ ì´ˆê³¼! 60ì´ˆ ëŒ€ê¸°... ({attempt+1}/{max_retries})")
                time.sleep(60)
                continue
            else:
                print(f"ğŸš¨ API ì—ëŸ¬: {response.text}")
                return None, None, None
        except Exception as e:
            print(f"ğŸš¨ ì—°ê²° ì—ëŸ¬: {e}")
            return None, None, None
    return None, None, None

# ==========================================
# 7. ê¸°ë¡ ê´€ë¦¬ í•¨ìˆ˜ (ìµœëŒ€ 300ê°œ ìœ ì§€)
# ==========================================

def get_processed_links(filename):
    if not os.path.exists(filename):
        return []
    with open(filename, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines()]

def save_processed_link(filename, link):
    links = get_processed_links(filename)
    if link not in links:
        links.append(link)
        if len(links) > MAX_HISTORY: # 300ê°œ ìœ ì§€
            links = links[-MAX_HISTORY:]
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("\n".join(links))

def get_global_titles():
    if not os.path.exists(GLOBAL_TITLE_FILE):
        return []
    with open(GLOBAL_TITLE_FILE, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines()]

def save_global_title(title):
    titles = get_global_titles()
    clean_title = re.sub(r'\s+', ' ', title).strip()
    
    if clean_title not in titles:
        titles.append(clean_title)
        if len(titles) > MAX_HISTORY: # 300ê°œ ìœ ì§€
            titles = titles[-MAX_HISTORY:]
        with open(GLOBAL_TITLE_FILE, 'w', encoding='utf-8') as f:
            f.write("\n".join(titles))

def is_similar_title(new_title, existing_titles):
    clean_new = re.sub(r'\s+', ' ', new_title).strip()
    
    for old_title in existing_titles:
        ratio = SequenceMatcher(None, clean_new, old_title).ratio()
        if ratio > 0.6: 
            print(f"ğŸš« ì¤‘ë³µ ê°ì§€ (ìœ ì‚¬ë„ {ratio:.2f}): {clean_new} <-> {old_title}")
            return True
    return False

# ==========================================
# 8. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================
if __name__ == "__main__":
    current_model = get_working_model()
    global_titles = get_global_titles()
    
    for category, rss_url, filename, default_source_name in RSS_SOURCES:
        print(f"\n--- [{category}] ---")
        try:
            feed = feedparser.parse(rss_url)
            if not feed.entries:
                print("ë‰´ìŠ¤ ì—†ìŒ (RSS ë¹„ì–´ìˆìŒ)")
                continue
            news = feed.entries[0]
        except:
            print("RSS íŒŒì‹± ì‹¤íŒ¨")
            continue
        
        # 1. ë§í¬ ì¤‘ë³µ ì²´í¬
        processed_links = get_processed_links(filename)
        if news.link in processed_links:
            print("ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬ì…ë‹ˆë‹¤.")
            continue

        # 2. ì œëª© ìœ ì‚¬ë„ ì²´í¬
        check_title = news.title if news.title else (news.description[:50] if hasattr(news, 'description') else "")
        if is_similar_title(check_title, global_titles):
            print("íŒ¨ìŠ¤: ë‹¤ë¥¸ ì†ŒìŠ¤ì—ì„œ ì´ë¯¸ ë‹¤ë£¬ ë‚´ìš©ì…ë‹ˆë‹¤.")
            save_processed_link(filename, news.link)
            continue

        print(f"âœ¨ ìƒˆ ë‰´ìŠ¤ ë°œê²¬: {news.title}")
        
        real_link = news.link
        content_for_ai = ""
        if hasattr(news, 'description'):
            content_for_ai = news.description
            if "í…”ë ˆê·¸ë¨" in category:
                urls = re.findall(r'(https?://\S+)', content_for_ai)
                if urls:
                    real_link = urls[0]
                    print(f"ğŸ”— í…”ë ˆê·¸ë¨ ì›ë¬¸ ë§í¬ ì¶”ì¶œë¨: {real_link}")

        body_text, img_lines, detected_source = summarize_news(current_model, news.title, real_link, content_for_ai)
        
        if body_text and img_lines:
            if "í…”ë ˆê·¸ë¨" in category:
                final_source_name = detected_source 
            else:
                final_source_name = default_source_name
            
            image_file = create_info_image(img_lines, final_source_name)
            
            try:
                media_id = None
                if image_file:
                    print(f"ğŸ–¼ï¸ ì¹´ë“œë‰´ìŠ¤ ìƒì„± ì™„ë£Œ (ì¶œì²˜: {final_source_name if final_source_name else 'ì—†ìŒ'})")
                    media = api.media_upload(image_file)
                    media_id = media.media_id
                
                if final_source_name:
                    final_tweet = f"{body_text}\n\nì¶œì²˜: {final_source_name}"
                else:
                    final_tweet = body_text 
                
                if len(final_tweet) > 12000:
                    final_tweet = final_tweet[:11995] + "..."

                if media_id:
                    response = client.create_tweet(text=final_tweet, media_ids=[media_id])
                else:
                    response = client.create_tweet(text=final_tweet)
                    
                tweet_id = response.data['id']
                print("âœ… ë©”ì¸ íŠ¸ìœ— ì—…ë¡œë“œ ì„±ê³µ!")
                
                reply_text = f"ğŸ”— ì›ë¬¸ ê¸°ì‚¬:\n{real_link}"
                client.create_tweet(text=reply_text, in_reply_to_tweet_id=tweet_id)
                print("âœ… ë§í¬ ëŒ“ê¸€ ì™„ë£Œ!")

                save_processed_link(filename, news.link)
                save_global_title(check_title)
                global_titles.append(re.sub(r'\s+', ' ', check_title).strip())
                
            except Exception as e:
                print(f"âŒ íŠ¸ìœ— ì „ì†¡ ì‹¤íŒ¨: {e}")
            
            if image_file and os.path.exists(image_file):
                os.remove(image_file)
        else:
            print("ğŸš¨ AI ìš”ì•½ ì‹¤íŒ¨")
    
        time.sleep(2)
