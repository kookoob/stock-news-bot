import feedparser
import tweepy
import requests
import os
import sys
import time
import textwrap
import re
import shutil
from difflib import SequenceMatcher
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont
from bs4 import BeautifulSoup  # â˜… ì›¹ í¬ë¡¤ë§ì„ ìœ„í•œ í•„ìˆ˜ ë„êµ¬

# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ==========================================
def get_clean_env(name):
    val = os.environ.get(name)
    if val is None: return None
    return val.strip().replace('\n', '').replace('\r', '').replace(' ', '')

GEMINI_API_KEY = get_clean_env("GEMINI_API_KEY")
CONSUMER_KEY = get_clean_env("CONSUMER_KEY")
CONSUMER_SECRET = get_clean_env("CONSUMER_SECRET")
ACCESS_TOKEN = get_clean_env("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = get_clean_env("ACCESS_TOKEN_SECRET")

# ==========================================
# 2. íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
# ==========================================
client = None
api = None
try:
    client = tweepy.Client(
        consumer_key=CONSUMER_KEY,
        consumer_secret=CONSUMER_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_TOKEN_SECRET
    )
    auth = tweepy.OAuth1UserHandler(
        CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET
    )
    api = tweepy.API(auth)
except Exception as e:
    print(f"âš ï¸ íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")

# ==========================================
# 3. ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
# ==========================================
RSS_SOURCES = [
    ("êµ­ì œì†ë³´(ì—°í•©)", "https://www.yna.co.kr/rss/international.xml", "last_link_yna_world.txt", "ì—°í•©ë‰´ìŠ¤"),
    ("ì „ìŸì†ë³´(êµ¬ê¸€)", "https://news.google.com/rss/search?q=ì „ìŸ+ì†ë³´+ë¯¸êµ­+ì´ë€&hl=ko&gl=KR&ceid=KR:ko", "last_link_google_war.txt", "Google News"),
    ("ë¯¸êµ­ì£¼ì‹(íˆ¬ì)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069", "last_link_us_investing.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸ˆìœµ)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664", "last_link_us_finance.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸°ìˆ )", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910", "last_link_us_tech.txt", "CNBC"),
    ("í•œêµ­ì£¼ì‹(í•œê²½)", "https://www.hankyung.com/feed/finance", "last_link_kr.txt", "í•œêµ­ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(Yahoo)", "https://finance.yahoo.com/news/rssindex", "last_link_yahoo.txt", "Yahoo Finance"),
    ("ë¯¸êµ­ì£¼ì‹(Tech)", "https://techcrunch.com/feed/", "last_link_techcrunch.txt", "TechCrunch"),
    ("í•œêµ­ì£¼ì‹(ë§¤ê²½)", "https://www.mk.co.kr/rss/50200011/", "last_link_mk.txt", "ë§¤ì¼ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Opinion)", "https://feeds.content.dowjones.io/public/rss/RSSOpinion", "last_link_wsj_op.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Market)", "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain", "last_link_wsj_mkt.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Economy)", "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed", "last_link_wsj_eco.txt", "WSJ"),
    ("ì†ë³´(í…”ë ˆê·¸ë¨)", "https://rsshub.app/telegram/channel/bornlupin", "last_link_bornlupin.txt", "Telegram"),
    ("í•œêµ­ì£¼ì‹(ì—°í•©)", "https://www.yna.co.kr/rss/economy.xml", "last_link_yna.txt", "ì—°í•©ë‰´ìŠ¤")
]

MAX_HISTORY = 2000
GLOBAL_TITLE_FILE = "processed_global_titles.txt"

# ==========================================
# 4. ì‹œê°„ ì œì–´ ë° í¬ë¡¤ë§ í•¨ìˆ˜
# ==========================================
def is_recent_news(entry):
    if not hasattr(entry, 'published_parsed') or not entry.published_parsed:
        return True
    try:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        current_time = datetime.now(timezone.utc)
        time_diff = current_time - published_time
        if time_diff > timedelta(hours=6):
            print(f"â³ [ì˜¤ë˜ëœ ë‰´ìŠ¤] 6ì‹œê°„ ê²½ê³¼ë¡œ ìŠ¤í‚µ: {time_diff}")
            return False
        return True
    except: return True

# â˜… [í•µì‹¬ ì¶”ê°€] ê¸°ì‚¬ ì›ë¬¸ ê¸ì–´ì˜¤ê¸° (í¬ë¡¤ë§)
def fetch_article_content(url):
    try:
        # ì‚¬ëŒì¸ ì²™ í•˜ê¸° ìœ„í•œ í—¤ë” (User-Agent)
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        
        # ì¸ì½”ë”© ìë™ ë³´ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
        response.encoding = response.apparent_encoding
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, í—¤ë”, í‘¸í„° ë“±)
        for script in soup(["script", "style", "header", "footer", "nav", "aside", "form"]):
            script.decompose()
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì „ëµ: <p> íƒœê·¸ ìœ„ì£¼ë¡œ ìˆ˜ì§‘
        paragraphs = soup.find_all('p')
        
        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥(ê´‘ê³ , ë§í¬ ë“±)ì€ ì œì™¸í•˜ê³  í•©ì¹˜ê¸°
        article_text = " ".join([p.get_text().strip() for p in paragraphs if len(p.get_text()) > 20])
        
        # ë§Œì•½ <p> íƒœê·¸ë¡œ ëª» ì°¾ì•˜ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê¸ì–´ì˜¤ê¸°
        if len(article_text) < 50:
             article_text = soup.get_text(separator=' ', strip=True)
             
        # í† í° ì ˆì•½ì„ ìœ„í•´ ìµœëŒ€ 4000ìê¹Œì§€ë§Œ ë°˜í™˜
        return article_text[:4000]
        
    except Exception as e:
        print(f"âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e} -> RSS ìš”ì•½ë³¸ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return None

# ==========================================
# 5. ì´ë¯¸ì§€ ë° AI ê´€ë ¨ í•¨ìˆ˜
# ==========================================
def create_gradient_background(width, height, start_color, end_color):
    base = Image.new('RGB', (width, height), start_color)
    top = Image.new('RGB', (width, height), end_color)
    mask = Image.new('L', (width, height))
    mask_data = []
    for y in range(height):
        mask_data.extend([int(255 * (y / height))] * width)
    mask.putdata(mask_data)
    base.paste(top, (0, 0), mask)
    return base

def create_info_image(text_lines, source_name):
    try:
        width, height = 1200, 675
        
        bg_start = (10, 25, 45)
        bg_end = (20, 40, 70)
        text_white = (245, 245, 250)
        text_gray = (180, 190, 210)
        accent_cyan = (0, 220, 255)
        title_box_bg = (0, 0, 0, 80)

        image = create_gradient_background(width, height, bg_start, bg_end)
        draw = ImageDraw.Draw(image, 'RGBA')

        try:
            font_title_main = ImageFont.truetype("font_bold.ttf", 60) 
            font_body = ImageFont.truetype("font_reg.ttf", 34)
            font_header = ImageFont.truetype("font_bold.ttf", 26)
            font_date = ImageFont.truetype("font_reg.ttf", 26)
        except:
            print("âš ï¸ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
            try:
                font_title_main = ImageFont.truetype("font.ttf", 60)
                font_body = ImageFont.truetype("font.ttf", 34)
                font_header = ImageFont.truetype("font.ttf", 26)
                font_date = ImageFont.truetype("font.ttf", 26)
            except: return None

        margin_x = 60
        current_y = 40

        header_text = "MARKET RADAR"
        if source_name: header_text += f" | {source_name}"
        
        draw.ellipse([(margin_x, current_y+8), (margin_x+12, current_y+20)], fill=accent_cyan)
        draw.text((margin_x + 25, current_y), header_text, font=font_header, fill=accent_cyan)

        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST)
        date_str = f"{now.year}.{now.month:02d}.{now.day:02d} | @marketradar0"
        
        date_bbox = draw.textbbox((0, 0), date_str, font=font_date)
        date_width = date_bbox[2] - date_bbox[0]
        draw.text((width - margin_x - date_width, current_y), date_str, font=font_date, fill=text_gray)
        
        current_y += 70

        for i, line in enumerate(text_lines):
            line = line.strip().replace("**", "").replace("##", "")
            if not line: continue

            if i == 0: 
                wrapped_title = textwrap.wrap(line, width=20)
                title_box_height = len(wrapped_title) * 85 + 30
                draw.rectangle([(margin_x - 20, current_y), (width - margin_x + 20, current_y + title_box_height)], fill=title_box_bg)
                
                current_y += 20
                for wl in wrapped_title:
                    draw.text((margin_x, current_y), wl, font=font_title_main, fill=text_white)
                    current_y += 85
                current_y += 40
            else: 
                bullet_text = "â–º"
                draw.text((margin_x, current_y + 2), bullet_text, font=font_header, fill=accent_cyan)
                
                wrapped_body = textwrap.wrap(line, width=40)
                for wl in wrapped_body:
                    draw.text((margin_x + 35, current_y), wl, font=font_body, fill=text_white)
                    current_y += 48
                current_y += 15

            if current_y > height - 60: break 

        draw.rectangle([(margin_x, height - 20), (width - margin_x, height - 18)], fill=accent_cyan)

        temp_filename = "temp_card_16_9.png"
        image.convert("RGB").save(temp_filename)
        return temp_filename
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì—ëŸ¬: {e}")
        return None

def get_working_model():
    return "gemini-1.5-flash"

def summarize_news(target_model, title, link, content_text=""):
    # í”„ë¡¬í”„íŠ¸: ì›ë¬¸ ë°ì´í„°ê°€ ë“¤ì–´ê°€ë¯€ë¡œ ì´ì œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ìš”êµ¬í•  ìˆ˜ ìˆìŒ
    prompt = f"""
    [ì—­í• ]
    ë„ˆëŠ” ê¸ˆìœµ íŒ©íŠ¸ ì²´í¬ ì „ë¬¸ê°€ë‹¤. 
    ì œê³µëœ [ë‰´ìŠ¤ ë‚´ìš©(Full Text)]ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ì •ë³´ë¥¼ ìš”ì•½í•´ë¼.

    [ì ˆëŒ€ ê·œì¹™]
    1. **ê¸°ì‚¬ ë³¸ë¬¸ì— ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ«ì(ê¸ˆì•¡, í¼ì„¼íŠ¸, ë‚ ì§œ)ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•´ë¼.**
    2. ë³¸ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆë¼.
    3. ë§Œì•½ ë³¸ë¬¸ í¬ë¡¤ë§ì— ì‹¤íŒ¨í•´ì„œ ë‚´ìš©ì´ ë¶€ì‹¤í•˜ë‹¤ë©´, ì œëª© ìœ„ì£¼ë¡œë§Œ ì‘ì„±í•´ë¼.

    [ì…ë ¥ ë°ì´í„°]
    ë‰´ìŠ¤ ì œëª©: {title}
    ë‰´ìŠ¤ ë§í¬: {link}
    ë‰´ìŠ¤ ë‚´ìš©(Full Text): {content_text}

    [ì¶œë ¥ ì–‘ì‹]
    ---BODY---
    (íŠ¸ìœ„í„° ë³¸ë¬¸ ì‘ì„±. í•œêµ­ì–´. ëª…ì‚¬í˜• ì¢…ê²°. í•µì‹¬ ìˆ˜ì¹˜ í¬í•¨.)
    ---IMAGE---
    (ì²« ì¤„ì€ ì œëª©. ë‚˜ë¨¸ì§€ëŠ” í•µì‹¬ ìš”ì•½ 3~5ì¤„. í•µì‹¬ ìˆ˜ì¹˜ í¬í•¨.)
    ---SOURCE---
    (ì–¸ë¡ ì‚¬ ì´ë¦„)
    """
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{target_model}:generateContent?key={GEMINI_API_KEY}"
    data = {"contents": [{"parts": [{"text": prompt}]}], "safetySettings": [{"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"}]}
    headers = {'Content-Type': 'application/json'}
    
    for _ in range(2): 
        try:
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                full_text = response.json()['candidates'][0]['content']['parts'][0]['text']
                if "---BODY---" in full_text and "---IMAGE---" in full_text:
                    parts = full_text.split("---IMAGE---")
                    body_raw = parts[0].replace("---BODY---", "").strip()
                    remaining = parts[1]
                    if "---SOURCE---" in remaining:
                        img_parts = remaining.split("---SOURCE---")
                        image_raw = img_parts[0].strip()
                        source_raw = img_parts[1].strip()
                    else: image_raw = remaining.strip(); source_raw = "Unknown"
                    body_part = body_raw.replace("**", "").replace("##", "")
                    image_lines = [re.sub(r"^[\-\*\â€¢\Â·\âœ…\âœ”\â–ª\â–«\â–º]+\s*", "", re.sub(r"^\d+\.\s+", "", l.strip().replace("**", "").replace("##", ""))) for l in image_raw.split('\n') if l.strip()]
                    source_name = source_raw.split('\n')[0].strip()
                    if "Unknown" in source_name or len(source_name) > 20: source_name = None
                    return body_part, image_lines, source_name
                return None, None, None
            elif response.status_code == 429: time.sleep(60); continue
            else: return None, None, None
        except: return None, None, None
    return None, None, None

# ==========================================
# 6. ê¸°ë¡ ê´€ë¦¬
# ==========================================
def get_processed_links(filename):
    if not os.path.exists(filename): return []
    with open(filename, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_processed_link(filename, link):
    links = get_processed_links(filename)
    clean_link = link.strip()
    if clean_link not in links:
        links.append(clean_link)
        if len(links) > MAX_HISTORY: links = links[-MAX_HISTORY:]
        with open(filename, 'w', encoding='utf-8') as f: f.write("\n".join(links))

def get_global_titles():
    if not os.path.exists(GLOBAL_TITLE_FILE): return []
    with open(GLOBAL_TITLE_FILE, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_global_title(title):
    titles = get_global_titles()
    clean_title = re.sub(r'\s+', ' ', title).strip()
    if clean_title not in titles:
        titles.append(clean_title)
        if len(titles) > MAX_HISTORY: titles = titles[-MAX_HISTORY:]
        with open(GLOBAL_TITLE_FILE, 'w', encoding='utf-8') as f: f.write("\n".join(titles))

def is_similar_title(new_title, existing_titles):
    clean_new = re.sub(r'\s+', ' ', new_title).strip()
    for old_title in existing_titles:
        if SequenceMatcher(None, clean_new, old_title).ratio() > 0.6: 
            print(f"ğŸš« ì¤‘ë³µ ê°ì§€ (ìœ ì‚¬ë„): {clean_new} <-> {old_title}")
            return True
    return False

# ==========================================
# 7. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================
if __name__ == "__main__":
    current_model = "gemini-1.5-flash"
    global_titles = get_global_titles()
    
    for category, rss_url, filename, default_source_name in RSS_SOURCES:
        print(f"\n--- [{category}] ---")
        
        try:
            feed = feedparser.parse(rss_url)
            if not feed.entries: print("ë‰´ìŠ¤ ì—†ìŒ"); continue
            news = feed.entries[0]
        except: print("RSS íŒŒì‹± ì‹¤íŒ¨"); continue
        
        if not is_recent_news(news): continue

        processed_links = get_processed_links(filename)
        if news.link.strip() in processed_links: 
            print("ğŸ’° [ë¹„ìš© ì ˆê°] ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬. API í˜¸ì¶œ ìƒëµ."); continue

        check_title = news.title if news.title else (news.description[:50] if hasattr(news, 'description') else "")
        
        if is_similar_title(check_title, global_titles):
            print("ğŸ’° [ë¹„ìš© ì ˆê°] ì¤‘ë³µ ë‚´ìš© ê°ì§€. API í˜¸ì¶œ ìƒëµ."); 
            save_processed_link(filename, news.link); 
            continue

        print(f"âœ¨ ìƒˆ ë‰´ìŠ¤ ë°œê²¬: {news.title}")
        print("ğŸŒ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
        
        # â˜… [í•µì‹¬] ì›ë¬¸ ê¸ì–´ì˜¤ê¸°
        real_link = news.link
        rss_summary = ""
        if hasattr(news, 'description'): rss_summary = news.description
        if "í…”ë ˆê·¸ë¨" in category: # í…”ë ˆê·¸ë¨ì€ í¬ë¡¤ë§ í•„ìš” ì—†ìŒ
            scraped_content = rss_summary
            urls = re.findall(r'(https?://\S+)', rss_summary)
            if urls: real_link = urls[0]
        else:
            # ì›¹ í¬ë¡¤ë§ ì‹œë„ -> ì‹¤íŒ¨ ì‹œ RSS ìš”ì•½ë³¸ ì‚¬ìš©
            scraped_text = fetch_article_content(real_link)
            scraped_content = scraped_text if scraped_text else rss_summary

        print("ğŸ¤– AI ë¶„ì„ ì‹œì‘...")
        body_text, img_lines, detected_source = summarize_news(current_model, news.title, real_link, scraped_content)
        
        if body_text and img_lines:
            final_source_name = detected_source if "í…”ë ˆê·¸ë¨" in category else default_source_name
            image_file = create_info_image(img_lines, final_source_name)
            
            try:
                media_id = None
                if image_file: 
                    print("ğŸ“¤ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì¤‘...")
                    media = api.media_upload(image_file)
                    media_id = media.media_id
                
                final_tweet = body_text
                
                if final_source_name: final_tweet += f"\n\nì¶œì²˜: {final_source_name}"
                if "ì£¼ì‹" in category and "#ì£¼ì‹" not in final_tweet: final_tweet += " #ì£¼ì‹"
                
                final_tweet += f"\n\nğŸ”— ì›ë¬¸: {real_link}"

                if len(final_tweet) > 11500: final_tweet = final_tweet[:11495] + "..."

                if media_id: response = client.create_tweet(text=final_tweet, media_ids=[media_id])
                else: response = client.create_tweet(text=final_tweet)
                
                print("âœ… ì—…ë¡œë“œ ì„±ê³µ")
                
                save_processed_link(filename, news.link)
                save_global_title(check_title)
                global_titles.append(re.sub(r'\s+', ' ', check_title).strip())
            except Exception as e: print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")
            if image_file and os.path.exists(image_file): os.remove(image_file)
        else: print("ğŸš¨ ìš”ì•½ ì‹¤íŒ¨")
        time.sleep(2)
