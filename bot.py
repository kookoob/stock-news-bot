import feedparser
import tweepy
import requests
import os
import sys
import time
import textwrap
import re
import shutil
from difflib import SequenceMatcher
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont
from bs4 import BeautifulSoup  # ì›¹/í…”ë ˆê·¸ë¨ í¬ë¡¤ë§ í•„ìˆ˜

# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ==========================================
def get_clean_env(name):
    val = os.environ.get(name)
    if val is None: return None
    return val.strip().replace('\n', '').replace('\r', '').replace(' ', '')

GEMINI_API_KEY = get_clean_env("GEMINI_API_KEY")
CONSUMER_KEY = get_clean_env("CONSUMER_KEY")
CONSUMER_SECRET = get_clean_env("CONSUMER_SECRET")
ACCESS_TOKEN = get_clean_env("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = get_clean_env("ACCESS_TOKEN_SECRET")

# ==========================================
# 2. íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
# ==========================================
client = None
api = None
try:
    client = tweepy.Client(
        consumer_key=CONSUMER_KEY,
        consumer_secret=CONSUMER_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_TOKEN_SECRET
    )
    auth = tweepy.OAuth1UserHandler(
        CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET
    )
    api = tweepy.API(auth)
except Exception as e:
    print(f"âš ï¸ íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")

# ==========================================
# 3. ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ (í…”ë ˆê·¸ë¨ ë§í¬ ìˆ˜ì •ë¨)
# ==========================================
RSS_SOURCES = [
    # â˜… [ìˆ˜ì •ë¨] RSS ëŒ€ì‹  ê³µì‹ ì›¹ í”„ë¦¬ë·° ì£¼ì†Œ ì‚¬ìš©
    ("ì†ë³´(í…”ë ˆê·¸ë¨)", "https://t.me/s/bornlupin", "last_link_bornlupin.txt", "Telegram"),

    ("êµ­ì œì†ë³´(ì—°í•©)", "https://www.yna.co.kr/rss/international.xml", "last_link_yna_world.txt", "ì—°í•©ë‰´ìŠ¤"),
    ("ì „ìŸì†ë³´(êµ¬ê¸€)", "https://news.google.com/rss/search?q=ì „ìŸ+ì†ë³´+ë¯¸êµ­+ì´ë€&hl=ko&gl=KR&ceid=KR:ko", "last_link_google_war.txt", "Google News"),
    ("ë¯¸êµ­ì£¼ì‹(íˆ¬ì)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069", "last_link_us_investing.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸ˆìœµ)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664", "last_link_us_finance.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸°ìˆ )", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910", "last_link_us_tech.txt", "CNBC"),
    ("í•œêµ­ì£¼ì‹(í•œê²½)", "https://www.hankyung.com/feed/finance", "last_link_kr.txt", "í•œêµ­ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(Yahoo)", "https://finance.yahoo.com/news/rssindex", "last_link_yahoo.txt", "Yahoo Finance"),
    ("ë¯¸êµ­ì£¼ì‹(Tech)", "https://techcrunch.com/feed/", "last_link_techcrunch.txt", "TechCrunch"),
    ("í•œêµ­ì£¼ì‹(ë§¤ê²½)", "https://www.mk.co.kr/rss/50200011/", "last_link_mk.txt", "ë§¤ì¼ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Opinion)", "https://feeds.content.dowjones.io/public/rss/RSSOpinion", "last_link_wsj_op.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Market)", "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain", "last_link_wsj_mkt.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Economy)", "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed", "last_link_wsj_eco.txt", "WSJ"),
    ("í•œêµ­ì£¼ì‹(ì—°í•©)", "https://www.yna.co.kr/rss/economy.xml", "last_link_yna.txt", "ì—°í•©ë‰´ìŠ¤")
]

MAX_HISTORY = 2000
GLOBAL_TITLE_FILE = "processed_global_titles.txt"

# ==========================================
# 4. í¬ë¡¤ë§ ë° ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# ==========================================
class SimpleNews:
    """RSSì™€ í…”ë ˆê·¸ë¨ ë°ì´í„°ë¥¼ í†µì¼ëœ í˜•íƒœë¡œ ë‹¤ë£¨ê¸° ìœ„í•œ ê°ì²´"""
    def __init__(self, title, link, description, published_parsed=None):
        self.title = title
        self.link = link
        self.description = description
        self.published_parsed = published_parsed

def is_recent_news(entry):
    if not hasattr(entry, 'published_parsed') or not entry.published_parsed:
        return True
    try:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        current_time = datetime.now(timezone.utc)
        time_diff = current_time - published_time
        if time_diff > timedelta(hours=6):
            print(f"â³ [ì˜¤ë˜ëœ ë‰´ìŠ¤] 6ì‹œê°„ ê²½ê³¼ë¡œ ìŠ¤í‚µ: {time_diff}")
            return False
        return True
    except: return True

def fetch_telegram_latest(url):
    """í…”ë ˆê·¸ë¨ t.me/s/ ì£¼ì†Œì—ì„œ ìµœì‹  ë©”ì‹œì§€ ì§ì ‘ í¬ë¡¤ë§"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë©”ì‹œì§€ ë˜í¼ë“¤ ì°¾ê¸°
        messages = soup.select('.tgme_widget_message_wrap')
        if not messages: return None
        
        # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€(ìµœì‹ )
        last_msg = messages[-1]
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_elem = last_msg.select_one('.tgme_widget_message_text')
        if not text_elem: return None # í…ìŠ¤íŠ¸ ì—†ëŠ” ì‚¬ì§„/ìŠ¤í‹°ì»¤ëŠ” íŒ¨ìŠ¤
        
        full_text = text_elem.get_text(separator="\n").strip()
        
        # ë§í¬ ì¶”ì¶œ (ë©”ì‹œì§€ ì‹œê°„ í´ë¦­ ì‹œ ì´ë™í•˜ëŠ” ê³ ìœ  ë§í¬)
        link_elem = last_msg.select_one('a.tgme_widget_message_date')
        if link_elem:
            post_link = link_elem['href']
        else:
            post_link = url # ë§í¬ ëª» ì°¾ìœ¼ë©´ ì±„ë„ ì£¼ì†Œë¡œ
            
        # ì œëª© ìƒì„± (ì²« ì¤„ í˜¹ì€ ì•ë¶€ë¶„)
        title = full_text.split('\n')[0]
        if len(title) > 50: title = title[:50] + "..."
        
        return SimpleNews(title, post_link, full_text)
        
    except Exception as e:
        print(f"âš ï¸ í…”ë ˆê·¸ë¨ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
        return None

def fetch_article_content(url):
    """ì¼ë°˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for script in soup(["script", "style", "header", "footer", "nav", "aside", "form"]):
            script.decompose()
        
        paragraphs = soup.find_all('p')
        article_text = " ".join([p.get_text().strip() for p in paragraphs if len(p.get_text()) > 20])
        
        if len(article_text) < 50:
             article_text = soup.get_text(separator=' ', strip=True)
             
        return article_text[:4000]
    except: return None

# ==========================================
# 5. ì´ë¯¸ì§€ ë° AI ê´€ë ¨ í•¨ìˆ˜
# ==========================================
def create_gradient_background(width, height, start_color, end_color):
    base = Image.new('RGB', (width, height), start_color)
    top = Image.new('RGB', (width, height), end_color)
    mask = Image.new('L', (width, height))
    mask_data = []
    for y in range(height):
        mask_data.extend([int(255 * (y / height))] * width)
    mask.putdata(mask_data)
    base.paste(top, (0, 0), mask)
    return base

def create_info_image(text_lines, source_name):
    try:
        width, height = 1200, 675
        bg_start = (10, 25, 45); bg_end = (20, 40, 70)
        text_white = (245, 245, 250); text_gray = (180, 190, 210)
        accent_cyan = (0, 220, 255); title_box_bg = (0, 0, 0, 80)

        image = create_gradient_background(width, height, bg_start, bg_end)
        draw = ImageDraw.Draw(image, 'RGBA')

        try:
            font_title_main = ImageFont.truetype("font_bold.ttf", 60) 
            font_body = ImageFont.truetype("font_reg.ttf", 34)
            font_header = ImageFont.truetype("font_bold.ttf", 26)
            font_date = ImageFont.truetype("font_reg.ttf", 26)
        except:
            print("âš ï¸ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
            try:
                font_title_main = ImageFont.truetype("font.ttf", 60)
                font_body = ImageFont.truetype("font.ttf", 34)
                font_header = ImageFont.truetype("font.ttf", 26)
                font_date = ImageFont.truetype("font.ttf", 26)
            except: return None

        margin_x = 60; current_y = 40
        header_text = "MARKET RADAR"; 
        if source_name: header_text += f" | {source_name}"
        
        draw.ellipse([(margin_x, current_y+8), (margin_x+12, current_y+20)], fill=accent_cyan)
        draw.text((margin_x + 25, current_y), header_text, font=font_header, fill=accent_cyan)

        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST)
        date_str = f"{now.year}.{now.month:02d}.{now.day:02d} | @marketradar0"
        
        date_bbox = draw.textbbox((0, 0), date_str, font=font_date)
        date_width = date_bbox[2] - date_bbox[0]
        draw.text((width - margin_x - date_width, current_y), date_str, font=font_date, fill=text_gray)
        current_y += 70

        for i, line in enumerate(text_lines):
            line = line.strip().replace("**", "").replace("##", "")
            if not line: continue
            if i == 0: 
                wrapped_title = textwrap.wrap(line, width=20)
                title_box_height = len(wrapped_title) * 85 + 30
                draw.rectangle([(margin_x - 20, current_y), (width - margin_x + 20, current_y + title_box_height)], fill=title_box_bg)
                current_y += 20
                for wl in wrapped_title:
                    draw.text((margin_x, current_y), wl, font=font_title_main, fill=text_white)
                    current_y += 85
                current_y += 40
            else: 
                bullet_text = "â–º"
                draw.text((margin_x, current_y + 2), bullet_text, font=font_header, fill=accent_cyan)
                wrapped_body = textwrap.wrap(line, width=40)
                for wl in wrapped_body:
                    draw.text((margin_x + 35, current_y), wl, font=font_body, fill=text_white)
                    current_y += 48
                current_y += 15
            if current_y > height - 60: break 
        draw.rectangle([(margin_x, height - 20), (width - margin_x, height - 18)], fill=accent_cyan)
        temp_filename = "temp_card_16_9.png"
        image.convert("RGB").save(temp_filename)
        return temp_filename
    except Exception as e: print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì—ëŸ¬: {e}"); return None

def get_working_model():
    return "gemini-1.5-flash"

def summarize_news(target_model, title, link, content_text=""):
    prompt = f"""
    [ì—­í• ] ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€.
    [ì…ë ¥ ë‰´ìŠ¤]
    ì œëª©: {title}
    ë‚´ìš©: {content_text}
    [í•„ìˆ˜ ê·œì¹™]
    1. ì„œë¡ (ì˜ˆ: "ë„¤, ìš”ì•½í•´ë“œë¦´ê²Œìš”") ì ˆëŒ€ ê¸ˆì§€. ë°”ë¡œ ê²°ê³¼ë§Œ ì¶œë ¥.
    2. ë³¸ë¬¸ì— ì—†ëŠ” ìˆ«ìëŠ” ì§€ì–´ë‚´ì§€ ë§ ê²ƒ.
    [ì¶œë ¥ í˜•ì‹ - ë°˜ë“œì‹œ ì´ í‹€ì„ ì§€í‚¬ ê²ƒ]
    ---BODY---
    (ì—¬ê¸°ì— íŠ¸ìœ— ë³¸ë¬¸ ì‘ì„±. í•œêµ­ì–´. ì´ëª¨ì§€ ì‚¬ìš©. í•´ì‹œíƒœê·¸ í¬í•¨)
    ---IMAGE---
    (ì—¬ê¸°ì— ì´ë¯¸ì§€ì— ë“¤ì–´ê°ˆ í…ìŠ¤íŠ¸ ì‘ì„±. ì²« ì¤„ì€ ì œëª©, ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½ 3ì¤„)
    ---SOURCE---
    (ì–¸ë¡ ì‚¬ ì´ë¦„. ëª¨ë¥´ë©´ Unknown)
    """
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{target_model}:generateContent?key={GEMINI_API_KEY}"
    data = {"contents": [{"parts": [{"text": prompt}]}], "safetySettings": [{"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"}]}
    headers = {'Content-Type': 'application/json'}
    for _ in range(2): 
        try:
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                full_text = response.json()['candidates'][0]['content']['parts'][0]['text']
                # â˜… [ìˆ˜ì •] Flash ëª¨ë¸ í˜•ì‹ ì˜¤ë¥˜ ë°©ì–´ ë¡œì§ í¬í•¨
                if "---BODY---" in full_text and "---IMAGE---" in full_text:
                    parts = full_text.split("---IMAGE---")
                    body_raw = parts[0].replace("---BODY---", "").strip()
                    remaining = parts[1]
                    if "---SOURCE---" in remaining:
                        img_parts = remaining.split("---SOURCE---")
                        image_raw = img_parts[0].strip()
                        source_raw = img_parts[1].strip()
                    else: image_raw = remaining.strip(); source_raw = "Unknown"
                    body_part = body_raw.replace("**", "").replace("##", "")
                    image_lines = [re.sub(r"^[\-\*\â€¢\Â·\âœ…\âœ”\â–ª\â–«\â–º]+\s*", "", l.strip()) for l in image_raw.split('\n') if l.strip()]
                    source_name = source_raw.split('\n')[0].strip()
                    return body_part, image_lines, source_name
                else: # í˜•ì‹ì´ ê¹¨ì¡Œì„ ë•Œ êµ¬ì œ
                    print("âš ï¸ í˜•ì‹ ì˜¤ë¥˜ ê°ì§€ -> ê°•ì œ ë³€í™˜ ì‹œë„")
                    body_part = full_text.replace("---BODY---", "").replace("---IMAGE---", "").strip()[:500]
                    image_lines = [title] + [body_part[:50] + "..."]
                    return body_part, image_lines, "Unknown"
            elif response.status_code == 429: time.sleep(60); continue
            else: return None, None, None
        except: return None, None, None
    return None, None, None

# ==========================================
# 6. ê¸°ë¡ ê´€ë¦¬
# ==========================================
def get_processed_links(filename):
    if not os.path.exists(filename): return []
    with open(filename, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_processed_link(filename, link):
    links = get_processed_links(filename)
    clean_link = link.strip()
    if clean_link not in links:
        links.append(clean_link)
        if len(links) > MAX_HISTORY: links = links[-MAX_HISTORY:]
        with open(filename, 'w', encoding='utf-8') as f: f.write("\n".join(links))

def get_global_titles():
    if not os.path.exists(GLOBAL_TITLE_FILE): return []
    with open(GLOBAL_TITLE_FILE, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_global_title(title):
    titles = get_global_titles()
    clean_title = re.sub(r'\s+', ' ', title).strip()
    if clean_title not in titles:
        titles.append(clean_title)
        if len(titles) > MAX_HISTORY: titles = titles[-MAX_HISTORY:]
        with open(GLOBAL_TITLE_FILE, 'w', encoding='utf-8') as f: f.write("\n".join(titles))

def is_similar_title(new_title, existing_titles):
    clean_new = re.sub(r'\s+', ' ', new_title).strip()
    for old_title in existing_titles:
        if SequenceMatcher(None, clean_new, old_title).ratio() > 0.6: 
            print(f"ğŸš« ì¤‘ë³µ ê°ì§€ (ìœ ì‚¬ë„): {clean_new} <-> {old_title}")
            return True
    return False

# ==========================================
# 7. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================
if __name__ == "__main__":
    current_model = "gemini-1.5-flash"
    global_titles = get_global_titles()
    
    for category, rss_url, filename, default_source_name in RSS_SOURCES:
        print(f"\n--- [{category}] ---")
        
        news = None
        # â˜… [í•µì‹¬] í…”ë ˆê·¸ë¨ì€ ë³„ë„ í¬ë¡¤ëŸ¬ ì‚¬ìš©, ë‚˜ë¨¸ì§€ëŠ” RSS ì‚¬ìš©
        if "t.me/s/" in rss_url:
             news = fetch_telegram_latest(rss_url)
             if not news: print("í…”ë ˆê·¸ë¨ ìƒˆ ë©”ì‹œì§€ ì—†ìŒ"); continue
        else:
            try:
                feed = feedparser.parse(rss_url)
                if not feed.entries: print("ë‰´ìŠ¤ ì—†ìŒ"); continue
                news = feed.entries[0]
                if not is_recent_news(news): continue # ì‹œê°„ ì²´í¬(RSSë§Œ)
            except: print("RSS íŒŒì‹± ì‹¤íŒ¨"); continue

        processed_links = get_processed_links(filename)
        if news.link.strip() in processed_links: 
            print("ğŸ’° [ë¹„ìš© ì ˆê°] ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬. API í˜¸ì¶œ ìƒëµ."); continue

        check_title = news.title if news.title else (news.description[:50] if hasattr(news, 'description') else "")
        if is_similar_title(check_title, global_titles):
            print("ğŸ’° [ë¹„ìš© ì ˆê°] ì¤‘ë³µ ë‚´ìš© ê°ì§€. API í˜¸ì¶œ ìƒëµ."); 
            save_processed_link(filename, news.link); continue

        print(f"âœ¨ ìƒˆ ë‰´ìŠ¤ ë°œê²¬: {news.title}")
        
        # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (í…”ë ˆê·¸ë¨ì€ ì´ë¯¸ ë³¸ë¬¸ì´ descriptionì— ìˆìŒ)
        real_link = news.link
        if "t.me/s/" in rss_url:
            scraped_content = news.description # í…”ë ˆê·¸ë¨ì€ ì´ê²Œ ë³¸ë¬¸
        else:
            print("ğŸŒ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
            rss_summary = news.description if hasattr(news, 'description') else ""
            scraped_text = fetch_article_content(real_link)
            scraped_content = scraped_text if scraped_text else rss_summary

        print("ğŸ¤– AI ë¶„ì„ ì‹œì‘...")
        body_text, img_lines, detected_source = summarize_news(current_model, news.title, real_link, scraped_content)
        
        if body_text and img_lines:
            final_source_name = detected_source if "í…”ë ˆê·¸ë¨" in category else default_source_name
            image_file = create_info_image(img_lines, final_source_name)
            
            try:
                media_id = None
                if image_file: 
                    print("ğŸ“¤ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì¤‘...")
                    media = api.media_upload(image_file)
                    media_id = media.media_id
                
                final_tweet = body_text
                if final_source_name: final_tweet += f"\n\nì¶œì²˜: {final_source_name}"
                if "ì£¼ì‹" in category and "#ì£¼ì‹" not in final_tweet: final_tweet += " #ì£¼ì‹"
                final_tweet += f"\n\nğŸ”— ì›ë¬¸: {real_link}"

                if len(final_tweet) > 11500: final_tweet = final_tweet[:11495] + "..."

                if media_id: response = client.create_tweet(text=final_tweet, media_ids=[media_id])
                else: response = client.create_tweet(text=final_tweet)
                
                print("âœ… ì—…ë¡œë“œ ì„±ê³µ")
                
                save_processed_link(filename, news.link)
                save_global_title(check_title)
                global_titles.append(re.sub(r'\s+', ' ', check_title).strip())
            except Exception as e: print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")
            if image_file and os.path.exists(image_file): os.remove(image_file)
        else: print("ğŸš¨ ìš”ì•½ ì‹¤íŒ¨")
        time.sleep(2)
