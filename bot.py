import feedparser
import tweepy
import requests
import os
import sys
import time
import textwrap
import re
import shutil
from difflib import SequenceMatcher
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont
from bs4 import BeautifulSoup

# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ==========================================
def get_clean_env(name):
    val = os.environ.get(name)
    if val is None: return None
    return val.strip().replace('\n', '').replace('\r', '').replace(' ', '')

GEMINI_API_KEY = get_clean_env("GEMINI_API_KEY")
CONSUMER_KEY = get_clean_env("CONSUMER_KEY")
CONSUMER_SECRET = get_clean_env("CONSUMER_SECRET")
ACCESS_TOKEN = get_clean_env("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = get_clean_env("ACCESS_TOKEN_SECRET")

# ==========================================
# 2. íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
# ==========================================
client = None
api = None
try:
    client = tweepy.Client(
        consumer_key=CONSUMER_KEY,
        consumer_secret=CONSUMER_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_TOKEN_SECRET
    )
    auth = tweepy.OAuth1UserHandler(
        CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET
    )
    api = tweepy.API(auth)
except Exception as e:
    print(f"âš ï¸ íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")

# ==========================================
# 3. ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
# ==========================================
RSS_SOURCES = [
    # í•˜ë‚˜ì°¨ì´ë‚˜ (í…”ë ˆê·¸ë¨)
    ("í•˜ë‚˜ì°¨ì´ë‚˜(China)", "https://t.me/s/HANAchina", "last_link_hana.txt", "Telegram"),
    
    # ë§ˆì´í´ ë²„ë¦¬ (Nitter ìš°íšŒ)
    ("ë§ˆì´í´ë²„ë¦¬(Burry)", "https://nitter.privacydev.net/michaeljburry/rss", "last_link_burry.txt", "Michael Burry"),

    # íŠ¸ëŸ¼í”„ íŠ¸ë£¨ìŠ¤ì†Œì…œ (API)
    ("íŠ¸ëŸ¼í”„(TruthSocial)", "https://truthsocial.com/@realDonaldTrump", "last_id_trump.txt", "Truth Social"),
    
    # ë¸”ë£¸ë²„ê·¸ (êµ¬ê¸€ë‰´ìŠ¤ í•„í„°ë§)
    ("ë¯¸êµ­ì£¼ì‹(ë¸”ë£¸ë²„ê·¸)", "https://news.google.com/rss/search?q=site:bloomberg.com+when:1d&hl=en-US&gl=US&ceid=US:en", "last_link_bloomberg.txt", "Bloomberg"),

    # í…”ë ˆê·¸ë¨ (ì†ë³´)
    ("ì†ë³´(í…”ë ˆê·¸ë¨)", "https://t.me/s/bornlupin", "last_link_bornlupin.txt", "Telegram"),

    # ì—°ì˜ˆë‰´ìŠ¤
    ("ì—°ì˜ˆë‰´ìŠ¤(ì—°í•©)", "https://www.yna.co.kr/rss/entertainment.xml", "last_link_yna_ent.txt", "ì—°í•©ë‰´ìŠ¤"),

    ("êµ­ì œì†ë³´(ì—°í•©)", "https://www.yna.co.kr/rss/international.xml", "last_link_yna_world.txt", "ì—°í•©ë‰´ìŠ¤"),
    ("ì „ìŸì†ë³´(êµ¬ê¸€)", "https://news.google.com/rss/search?q=ì „ìŸ+ì†ë³´+ë¯¸êµ­+ì´ë€&hl=ko&gl=KR&ceid=KR:ko", "last_link_google_war.txt", "Google News"),
    ("ë¯¸êµ­ì£¼ì‹(íˆ¬ì)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069", "last_link_us_investing.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸ˆìœµ)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664", "last_link_us_finance.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸°ìˆ )", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910", "last_link_us_tech.txt", "CNBC"),
    ("í•œêµ­ì£¼ì‹(í•œê²½)", "https://www.hankyung.com/feed/finance", "last_link_kr.txt", "í•œêµ­ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(Yahoo)", "https://finance.yahoo.com/news/rssindex", "last_link_yahoo.txt", "Yahoo Finance"),
    ("ë¯¸êµ­ì£¼ì‹(Tech)", "https://techcrunch.com/feed/", "last_link_techcrunch.txt", "TechCrunch"),
    ("í•œêµ­ì£¼ì‹(ë§¤ê²½)", "https://www.mk.co.kr/rss/50200011/", "last_link_mk.txt", "ë§¤ì¼ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Opinion)", "https://feeds.content.dowjones.io/public/rss/RSSOpinion", "last_link_wsj_op.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Market)", "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain", "last_link_wsj_mkt.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Economy)", "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed", "last_link_wsj_eco.txt", "WSJ"),
    ("í•œêµ­ì£¼ì‹(ì—°í•©)", "https://www.yna.co.kr/rss/economy.xml", "last_link_yna.txt", "ì—°í•©ë‰´ìŠ¤")
]

MAX_HISTORY = 2000
GLOBAL_TITLE_FILE = "processed_global_titles.txt" # ì œëª©/ì›ë¬¸ í•´ì‹œ ì €ì¥
GLOBAL_SUMMARY_FILE = "processed_ai_summaries.txt" # â˜… AI ìš”ì•½ë³¸ ì €ì¥ (ìƒˆë¡œ ì¶”ê°€)

# ==========================================
# 4. í¬ë¡¤ë§ ë° ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# ==========================================
class SimpleNews:
    def __init__(self, title, link, description, published_parsed=None, image_url=None):
        self.title = title
        self.link = link
        self.description = description
        self.published_parsed = published_parsed
        self.image_url = image_url

def is_recent_news(entry):
    if not hasattr(entry, 'published_parsed') or not entry.published_parsed: return True
    try:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        current_time = datetime.now(timezone.utc)
        time_diff = current_time - published_time
        if time_diff > timedelta(hours=6):
            print(f"â³ [ì˜¤ë˜ëœ ë‰´ìŠ¤] 6ì‹œê°„ ê²½ê³¼ë¡œ ìŠ¤í‚µ: {time_diff}")
            return False
        return True
    except: return True

def download_image_from_url(url, save_path="temp_origin.jpg"):
    try:
        if "google" in url or "gstatic" in url:
            print("ğŸš« êµ¬ê¸€ ê¸°ë³¸ ì´ë¯¸ì§€ëŠ” ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ")
            return None
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, stream=True, timeout=10)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                response.raw.decode_content = True
                shutil.copyfileobj(response.raw, f)
            return save_path
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None

def fetch_truth_social_latest(url):
    try:
        TRUMP_ACCOUNT_ID = "107780213600000000"
        api_url = f"https://truthsocial.com/api/v1/accounts/{TRUMP_ACCOUNT_ID}/statuses?exclude_replies=true&only_media=false"
        headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/json'}
        response = requests.get(api_url, headers=headers, timeout=10)
        if response.status_code != 200: return None
        posts = response.json()
        if not posts: return None
        latest_post = posts[0]
        post_id = latest_post.get('id')
        content_html = latest_post.get('content', '')
        created_at_str = latest_post.get('created_at')
        image_url = None
        media_attachments = latest_post.get('media_attachments', [])
        if media_attachments: image_url = media_attachments[0].get('url')
        soup = BeautifulSoup(content_html, 'html.parser')
        full_text = soup.get_text(separator="\n").strip()
        post_link = f"https://truthsocial.com/@realDonaldTrump/posts/{post_id}"
        title = full_text.split('\n')[0]
        if len(title) > 50: title = title[:50] + "..."
        if not title: title = "íŠ¸ëŸ¼í”„ íŠ¸ë£¨ìŠ¤ì†Œì…œ ìµœì‹  í¬ìŠ¤íŒ…"
        try:
            post_time = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
            if (datetime.now(timezone.utc) - post_time) > timedelta(hours=6): return None
        except: pass
        return SimpleNews(title, post_link, full_text, image_url=image_url)
    except Exception as e:
        print(f"âš ï¸ íŠ¸ë£¨ìŠ¤ì†Œì…œ ì—ëŸ¬: {e}")
        return None

def fetch_telegram_latest(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        messages = soup.select('.tgme_widget_message_wrap')
        if not messages: return None
        last_msg = messages[-1]
        text_elem = last_msg.select_one('.tgme_widget_message_text')
        full_text = ""
        if text_elem: full_text = text_elem.get_text(separator="\n").strip()
        image_url = None
        photo_div = last_msg.select_one('.tgme_widget_message_photo_wrap')
        if photo_div:
            style = photo_div.get('style', '')
            match = re.search(r"url\('?(.*?)'?\)", style)
            if match: image_url = match.group(1)
        link_elem = last_msg.select_one('a.tgme_widget_message_date')
        post_link = link_elem['href'] if link_elem else url
        title = full_text.split('\n')[0] if full_text else "í…”ë ˆê·¸ë¨ ì´ë¯¸ì§€ í¬ìŠ¤íŠ¸"
        if len(title) > 50: title = title[:50] + "..."
        return SimpleNews(title, post_link, full_text, image_url=image_url)
    except Exception as e:
        print(f"âš ï¸ í…”ë ˆê·¸ë¨ ì—ëŸ¬: {e}")
        return None

def fetch_article_content_and_image(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        image_url = None
        og_image = soup.find('meta', property='og:image')
        if og_image: 
            found_url = og_image.get('content')
            if found_url and ("google" not in found_url and "gstatic" not in found_url):
                image_url = found_url
            else:
                print("ğŸš« êµ¬ê¸€/ê¸°ë³¸ ë¡œê³  ê°ì§€ë˜ì–´ ì´ë¯¸ì§€ ìŠ¤í‚µí•¨")

        for script in soup(["script", "style", "header", "footer", "nav", "aside", "form"]):
            script.decompose()
        paragraphs = soup.find_all('p')
        article_text = " ".join([p.get_text().strip() for p in paragraphs if len(p.get_text()) > 20])
        if len(article_text) < 100: article_text = soup.get_text(separator=' ', strip=True)
        return article_text[:4000], image_url
    except: return None, None

# ==========================================
# 5. ì´ë¯¸ì§€ ìƒì„± (ë””ìì¸)
# ==========================================
def create_gradient_background(width, height, start_color, end_color):
    base = Image.new('RGB', (width, height), start_color)
    top = Image.new('RGB', (width, height), end_color)
    mask = Image.new('L', (width, height))
    mask_data = []
    for y in range(height): mask_data.extend([int(255 * (y / height))] * width)
    mask.putdata(mask_data)
    base.paste(top, (0, 0), mask)
    return base

def create_info_image(text_lines, source_name):
    try:
        width, height = 1200, 675
        bg_start = (10, 25, 45); bg_end = (20, 40, 70)
        text_white = (245, 245, 250); text_gray = (180, 190, 210)
        accent_cyan = (0, 220, 255); title_box_bg = (0, 0, 0, 80)
        image = create_gradient_background(width, height, bg_start, bg_end)
        draw = ImageDraw.Draw(image, 'RGBA')
        try:
            font_title_main = ImageFont.truetype("font_bold.ttf", 60)
            font_body = ImageFont.truetype("font_reg.ttf", 34)
            font_header = ImageFont.truetype("font_bold.ttf", 26)
            font_date = ImageFont.truetype("font_reg.ttf", 26)
        except:
            try:
                font_title_main = ImageFont.truetype("font.ttf", 60)
                font_body = ImageFont.truetype("font.ttf", 34)
                font_header = ImageFont.truetype("font.ttf", 26)
                font_date = ImageFont.truetype("font.ttf", 26)
            except: return None
        margin_x = 60; current_y = 40
        header_text = "MARKET RADAR"; 
        
        if source_name and source_name != "Telegram": 
            header_text += f" | {source_name}"
            
        draw.ellipse([(margin_x, current_y+8), (margin_x+12, current_y+20)], fill=accent_cyan)
        draw.text((margin_x + 25, current_y), header_text, font=font_header, fill=accent_cyan)
        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST)
        date_str = f"{now.year}.{now.month:02d}.{now.day:02d} | @marketradar0"
        date_bbox = draw.textbbox((0, 0), date_str, font=font_date)
        date_width = date_bbox[2] - date_bbox[0]
        draw.text((width - margin_x - date_width, current_y), date_str, font=font_date, fill=text_gray)
        current_y += 70
        for i, line in enumerate(text_lines):
            clean_line = re.sub(r"^[\W_]+", "", line.strip()) 
            clean_line = clean_line.replace("**", "").replace("##", "")
            if not clean_line: continue
            if i == 0: 
                wrapped_title = textwrap.wrap(clean_line, width=20)
                title_box_height = len(wrapped_title) * 85 + 30
                draw.rectangle([(margin_x - 20, current_y), (width - margin_x + 20, current_y + title_box_height)], fill=title_box_bg)
                current_y += 20
                for wl in wrapped_title:
                    draw.text((margin_x, current_y), wl, font=font_title_main, fill=text_white)
                    current_y += 85
                current_y += 40
            else: 
                bullet_y = current_y + 12
                draw.rectangle([margin_x, bullet_y, margin_x + 10, bullet_y + 10], fill=accent_cyan)
                wrapped_body = textwrap.wrap(clean_line, width=40)
                for wl in wrapped_body:
                    draw.text((margin_x + 35, current_y), wl, font=font_body, fill=text_white)
                    current_y += 48
                current_y += 15
            if current_y > height - 60: break 
        draw.rectangle([(margin_x, height - 20), (width - margin_x, height - 18)], fill=accent_cyan)
        temp_filename = "temp_card_16_9.png"
        image.convert("RGB").save(temp_filename)
        return temp_filename
    except: return None

# ==========================================
# 6. AI ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸
# ==========================================
def get_working_model():
    print("ğŸ¤– AI ëª¨ë¸ ì¡°íšŒ ì¤‘...")
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GEMINI_API_KEY}"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            models = [m['name'].replace('models/', '') for m in data.get('models', [])]
            priorities = ["gemini-1.5-pro", "gemini-1.5-pro-latest", "gemini-1.5-pro-001", "gemini-pro"]
            for p in priorities:
                if p in models: return p
            for m in data.get('models', []):
                if 'generateContent' in m.get('supportedGenerationMethods', []):
                    return m['name'].replace('models/', '')
    except: pass
    return "gemini-pro"

def summarize_news(target_model, title, link, content_text=""):
    prompt = f"""
    [ì§€ì‹œì‚¬í•­]
    ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¸ìœ„í„° ê²Œì‹œê¸€ê³¼ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.
    
    [ì…ë ¥ ë°ì´í„°]
    ì œëª©: {title}
    ë§í¬: {link}
    ë‚´ìš©: {content_text}

    [í•„ìˆ˜ ê·œì¹™]
    1. ê°ì • ë°°ì œ, ê±´ì¡°í•œ ë‰´ìŠ¤ í†¤ ìœ ì§€.
    2. ë§íˆ¬ëŠ” '~í•¨', '~ìŒ' ë“± ëª…ì‚¬í˜• ì¢…ê²°. (ì¡´ëŒ“ë§ ê¸ˆì§€)
    3. ëŠë‚Œí‘œ(!) ì‚¬ìš© ê¸ˆì§€.
    4. ë¬´ì¡°ê±´ í•œêµ­ì–´ ì‘ì„±.
    5. íŠ¸ìœ„í„° ë³¸ë¬¸ì€ âœ… ë¦¬ìŠ¤íŠ¸ í˜•ì‹.
    6. **í‹°ì»¤ì™€ í•´ì‹œíƒœê·¸ì— ê´„í˜¸() ì‚¬ìš© ê¸ˆì§€. ê³µë°±ìœ¼ë¡œ êµ¬ë¶„.** (ì˜ˆ: $TSLA #ì „ê¸°ì°¨)
    7. ì´ë¯¸ì§€ëŠ” ì œëª© ì œì™¸ ìµœëŒ€ 7ì¤„.

    [ì¶œë ¥ í¬ë§·]
    ---BODY---
    (ì´ëª¨ì§€) (í•œêµ­ì–´ ì œëª©)
    
    âœ… (ë‚´ìš© 1)
    âœ… (ë‚´ìš© 2)
    âœ… (ë‚´ìš© 3)
    
    $AAA #BBB #CCC

    ---IMAGE---
    (í•œêµ­ì–´ ì œëª©)
    (ìš”ì•½ 1)
    (ìš”ì•½ 2)

    ---SOURCE---
    (ì–¸ë¡ ì‚¬)
    """
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{target_model}:generateContent?key={GEMINI_API_KEY}"
    safety_settings = [{"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"}]
    data = {"contents": [{"parts": [{"text": prompt}]}], "safetySettings": safety_settings}
    
    for _ in range(2): 
        try:
            response = requests.post(url, headers={'Content-Type': 'application/json'}, json=data)
            if response.status_code != 200: continue
            full_text = response.json()['candidates'][0]['content']['parts'][0]['text']
            if "---BODY---" in full_text and "---IMAGE---" in full_text:
                parts = full_text.split("---IMAGE---")
                body_raw = parts[0].replace("---BODY---", "").strip()
                remaining = parts[1]
                if "---SOURCE---" in remaining:
                    img_parts = remaining.split("---SOURCE---")
                    image_raw = img_parts[0].strip()
                    source_raw = img_parts[1].strip()
                else: image_raw = remaining.strip(); source_raw = "Unknown"
                
                body_part = body_raw.replace("**", "").replace("##", "")
                image_lines = [l.strip() for l in image_raw.split('\n') if l.strip()]
                source_name = source_raw.split('\n')[0].strip()
                return body_part, image_lines, source_name
            else:
                body_part = full_text.strip()[:500]
                image_lines = [title] + [body_part[:50] + "..."]
                return body_part, image_lines, "Unknown"
        except: continue
    return None, None, None

# ==========================================
# 7. ë©”ì¸ ì‹¤í–‰ ë¡œì§ (â˜…3ë‹¨ê³„ ì¤‘ë³µ í•„í„°ë§â˜…)
# ==========================================
def get_file_lines(filename):
    if not os.path.exists(filename): return []
    with open(filename, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_file_line(filename, line):
    lines = get_file_lines(filename)
    # ì¤„ë°”ê¿ˆ ë° ê³µë°± ì œê±°ëœ ìƒíƒœë¡œ ì €ì¥
    clean_line = re.sub(r'\s+', ' ', line).strip()
    if clean_line not in lines:
        lines.append(clean_link if 'http' in line else clean_line)
        if len(lines) > MAX_HISTORY: lines = lines[-MAX_HISTORY:]
        with open(filename, 'w', encoding='utf-8') as f: f.write("\n".join(lines))

def normalize_text(text):
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜, ë‹¨ì–´ ì„¸íŠ¸ ë°˜í™˜ (ë¹„êµìš©)
    text = re.sub(r'[^\w\s]', '', text.lower())
    return set(text.split())

def is_duplicate_content(new_text, history_lines, threshold=0.6):
    """
    í…ìŠ¤íŠ¸ ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬ (Jaccard & SequenceMatcher)
    new_text: ë¹„êµí•  ìƒˆ í…ìŠ¤íŠ¸ (ë³¸ë¬¸ ë˜ëŠ” AI ìš”ì•½ë³¸)
    history_lines: ê¸°ì¡´ ì €ì¥ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    threshold: ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•  ìœ ì‚¬ë„ ê¸°ì¤€ (0.6 = 60%)
    """
    if not new_text or len(new_text) < 10: return False
    
    new_words = normalize_text(new_text)
    if len(new_words) < 3: return False 

    # ìµœì‹  ê¸°ë¡ë¶€í„° ì—­ìˆœ ë¹„êµ (ì†ë„ í–¥ìƒ)
    for old_text in reversed(history_lines):
        # 1. ë‹¨ì–´ êµì§‘í•© ê²€ì‚¬ (ë¹ ë¦„)
        old_words = normalize_text(old_text)
        if len(old_words) == 0: continue
        
        intersection = len(new_words & old_words)
        union = len(new_words | old_words)
        jaccard_sim = intersection / union if union > 0 else 0
        
        if jaccard_sim > 0.4: # ë‹¨ì–´ê°€ 40% ì´ìƒ ê²¹ì¹˜ë©´ ì˜ì‹¬
            # 2. ì •ë°€ ë¬¸ìì—´ ë¹„êµ (ëŠë¦¼, ì •í™•)
            seq_sim = SequenceMatcher(None, new_text, old_text).ratio()
            if seq_sim > threshold:
                print(f"ğŸš« [ì¤‘ë³µ ê°ì§€] ìœ ì‚¬ë„ {seq_sim:.2f} | '{new_text[:30]}...'")
                return True
    return False

if __name__ == "__main__":
    current_model = get_working_model()
    
    # ì „ì—­ ê¸°ë¡ ë¡œë“œ
    global_titles = get_file_lines(GLOBAL_TITLE_FILE)
    global_summaries = get_file_lines(GLOBAL_SUMMARY_FILE) # AI ìš”ì•½ë³¸ ê¸°ë¡
    
    for category, rss_url, filename, default_source_name in RSS_SOURCES:
        print(f"\n--- [{category}] ---")
        
        news = None
        is_telegram = "t.me" in rss_url

        if "truthsocial.com" in rss_url: 
             news = fetch_truth_social_latest(rss_url)
             if not news: print("íŠ¸ë£¨ìŠ¤ì†Œì…œ ìƒˆ ê¸€ ì—†ìŒ"); continue
        elif "t.me/s/" in rss_url: 
             news = fetch_telegram_latest(rss_url)
             if not news: print("í…”ë ˆê·¸ë¨ ì—†ìŒ"); continue
        else:
            try:
                feed = feedparser.parse(rss_url)
                if not feed.entries: print("ë‰´ìŠ¤ ì—†ìŒ"); continue
                news = feed.entries[0]
                if not is_recent_news(news): continue 
            except: print("RSS ì‹¤íŒ¨"); continue

        # [í•„í„° 1] ë§í¬ ê²€ì‚¬ (ì •í™•íˆ ê°™ì€ URL)
        processed_links = get_file_lines(filename)
        if news.link.strip() in processed_links: 
            print("ğŸ’° ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬ (Link match)"); continue

        # [í•„í„° 2] ì›ë¬¸/ì œëª© ë‚´ìš© ê²€ì‚¬ (ìœ ì‚¬í•œ ì œëª©/ë³¸ë¬¸)
        # í…”ë ˆê·¸ë¨ì€ ì œëª©ì´ ë³¸ë¬¸ê³¼ ê°™ìœ¼ë¯€ë¡œ ë³¸ë¬¸ ë¹„êµ íš¨ê³¼
        check_content = news.title if news.title else (news.description[:100] if hasattr(news, 'description') else "")
        if is_duplicate_content(check_content, global_titles, threshold=0.55):
            # ë§í¬ë§Œ ë‹¤ë¥´ê³  ë‚´ìš©ì´ ê°™ìœ¼ë©´, ë§í¬ë„ ì²˜ë¦¬ëœ ê±¸ë¡œ ì €ì¥í•´ë²„ë¦¼
            save_file_line(filename, news.link)
            continue

        print(f"âœ¨ ìƒˆ ë‰´ìŠ¤ ë°œê²¬: {news.title[:30]}...")
        
        real_link = news.link
        original_image_url = None
        
        if "truthsocial.com" in rss_url:
            scraped_content = news.description
            original_image_url = news.image_url
        elif "t.me/s/" in rss_url:
            scraped_content = news.description
            original_image_url = news.image_url
        elif "nitter" in rss_url:
            scraped_content = news.description 
        else:
            print("ğŸŒ í¬ë¡¤ë§ ì¤‘...")
            rss_summary = news.description if hasattr(news, 'description') else ""
            scraped_text, found_img_url = fetch_article_content_and_image(real_link)
            scraped_content = scraped_text if (scraped_text and len(scraped_text) > 50) else rss_summary
            original_image_url = found_img_url

        print("ğŸ¤– AI ë¶„ì„ ì‹œì‘...")
        body_text, img_lines, detected_source = summarize_news(current_model, news.title, real_link, scraped_content)
        
        if body_text and img_lines:
            # [í•„í„° 3] AI ìš”ì•½ë³¸ ê²€ì‚¬ (í•µì‹¬ ë‚´ìš© ì¤‘ë³µ í™•ì¸)
            # AIê°€ ìš”ì•½í•œ ë‚´ìš©ì´ ê¸°ì¡´ ìš”ì•½ë“¤ê³¼ ë¹„ìŠ·í•˜ë©´ ì—¬ê¸°ì„œ ìµœì¢… ìŠ¤í‚µ
            if is_duplicate_content(body_text, global_summaries, threshold=0.6):
                print("ğŸš¨ [AI ìš”ì•½ ì¤‘ë³µ] ë‚´ìš©ì´ ê¸°ì¡´ íŠ¸ìœ—ê³¼ ë™ì¼í•˜ì—¬ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                # ì´ê²ƒë„ ì²˜ë¦¬ëœ ê±¸ë¡œ ì €ì¥í•˜ì—¬ ë‹¤ì‹œ ì‹œë„ ì•ˆ í•˜ê²Œ í•¨
                save_file_line(filename, news.link)
                continue

            final_source_name = detected_source if is_telegram else default_source_name
            if "TruthSocial" in category: final_source_name = "Truth Social (Donald Trump)"
            if "Burry" in category: final_source_name = "Michael Burry (Twitter)"
            if is_telegram: final_source_name = "Telegram"
                
            summary_card_file = create_info_image(img_lines, final_source_name)
            
            original_image_file = None
            if original_image_url:
                print("ğŸ–¼ï¸ ì›ë³¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                original_image_file = download_image_from_url(original_image_url)

            try:
                media_ids = []
                if summary_card_file: 
                    media1 = api.media_upload(summary_card_file)
                    media_ids.append(media1.media_id)
                if original_image_file:
                    try:
                        media2 = api.media_upload(original_image_file)
                        media_ids.append(media2.media_id)
                    except: pass
                
                final_tweet = body_text
                
                if final_source_name and not is_telegram:
                    final_tweet += f"\n\nì¶œì²˜: {final_source_name}"
                
                final_tweet += " #ë§ˆì¼“ë ˆì´ë”"
                if len(final_tweet) > 12000: final_tweet = final_tweet[:11995] + "..."

                if media_ids: response = client.create_tweet(text=final_tweet, media_ids=media_ids)
                else: response = client.create_tweet(text=final_tweet)
                
                print("âœ… ë©”ì¸ íŠ¸ìœ— ì„±ê³µ")

                # ì„±ê³µ í›„ ë°ì´í„° ì €ì¥
                save_file_line(filename, news.link) # ë§í¬ ì €ì¥
                save_file_line(GLOBAL_TITLE_FILE, check_content) # ì œëª©/ì›ë¬¸ ì•ë¶€ë¶„ ì €ì¥
                
                # â˜… AI ìš”ì•½ë³¸ë„ ì €ì¥ (ë‹¤ìŒ ì¤‘ë³µ ê²€ì‚¬ ë•Œ ì‚¬ìš©)
                # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë°”ê¿”ì„œ í•œ ì¤„ë¡œ ì €ì¥
                clean_summary = re.sub(r'\s+', ' ', body_text).strip()
                with open(GLOBAL_SUMMARY_FILE, 'a', encoding='utf-8') as f:
                    f.write(clean_summary + "\n")
                
                print("â³ ë„ë°° ë°©ì§€: 3ë¶„ ëŒ€ê¸°...")
                time.sleep(180)

            except Exception as e: print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")
            
            if summary_card_file and os.path.exists(summary_card_file): os.remove(summary_card_file)
            if original_image_file and os.path.exists(original_image_file): os.remove(original_image_file)
        else: print("ğŸš¨ ìš”ì•½ ì‹¤íŒ¨")
        time.sleep(2)
