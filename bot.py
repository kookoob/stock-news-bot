import feedparser
import tweepy
import requests
import os
import sys
import time
import textwrap
import re
import shutil
from difflib import SequenceMatcher
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont

# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ==========================================
def get_clean_env(name):
    val = os.environ.get(name)
    if val is None: return None
    return val.strip().replace('\n', '').replace('\r', '').replace(' ', '')

GEMINI_API_KEY = get_clean_env("GEMINI_API_KEY")
CONSUMER_KEY = get_clean_env("CONSUMER_KEY")
CONSUMER_SECRET = get_clean_env("CONSUMER_SECRET")
ACCESS_TOKEN = get_clean_env("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = get_clean_env("ACCESS_TOKEN_SECRET")

# ==========================================
# 2. íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
# ==========================================
client = None
api = None
try:
    client = tweepy.Client(
        consumer_key=CONSUMER_KEY,
        consumer_secret=CONSUMER_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_TOKEN_SECRET
    )
    auth = tweepy.OAuth1UserHandler(
        CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET
    )
    api = tweepy.API(auth)
except Exception as e:
    print(f"âš ï¸ íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")

# ==========================================
# 3. ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
# ==========================================
RSS_SOURCES = [
    # êµ­ì œ/ì „ìŸ ì†ë³´
    ("êµ­ì œì†ë³´(ì—°í•©)", "https://www.yna.co.kr/rss/international.xml", "last_link_yna_world.txt", "ì—°í•©ë‰´ìŠ¤"),
    ("ì „ìŸì†ë³´(êµ¬ê¸€)", "https://news.google.com/rss/search?q=ì „ìŸ+ì†ë³´+ë¯¸êµ­+ì´ë€&hl=ko&gl=KR&ceid=KR:ko", "last_link_google_war.txt", "Google News"),

    # ê²½ì œ/ì£¼ì‹ ë‰´ìŠ¤
    ("ë¯¸êµ­ì£¼ì‹(íˆ¬ì)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069", "last_link_us_investing.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸ˆìœµ)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664", "last_link_us_finance.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸°ìˆ )", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910", "last_link_us_tech.txt", "CNBC"),
    ("í•œêµ­ì£¼ì‹(í•œê²½)", "https://www.hankyung.com/feed/finance", "last_link_kr.txt", "í•œêµ­ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(Yahoo)", "https://finance.yahoo.com/news/rssindex", "last_link_yahoo.txt", "Yahoo Finance"),
    ("ë¯¸êµ­ì£¼ì‹(Tech)", "https://techcrunch.com/feed/", "last_link_techcrunch.txt", "TechCrunch"),
    ("í•œêµ­ì£¼ì‹(ë§¤ê²½)", "https://www.mk.co.kr/rss/50200011/", "last_link_mk.txt", "ë§¤ì¼ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Opinion)", "https://feeds.content.dowjones.io/public/rss/RSSOpinion", "last_link_wsj_op.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Market)", "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain", "last_link_wsj_mkt.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Economy)", "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed", "last_link_wsj_eco.txt", "WSJ"),
    ("ì†ë³´(í…”ë ˆê·¸ë¨)", "https://rsshub.app/telegram/channel/bornlupin", "last_link_bornlupin.txt", "Telegram"),
    ("í•œêµ­ì£¼ì‹(ì—°í•©)", "https://www.yna.co.kr/rss/economy.xml", "last_link_yna.txt", "ì—°í•©ë‰´ìŠ¤")
]

# â˜… [ìˆ˜ì •] ê¸°ì–µí•  íˆìŠ¤í† ë¦¬ ê°œìˆ˜ (2000ê°œë¡œ ìƒí–¥)
MAX_HISTORY = 2000
GLOBAL_TITLE_FILE = "processed_global_titles.txt"

# ==========================================
# 4. ì‹œê°„ ì œì–´ í•¨ìˆ˜ (6ì‹œê°„ ì´ë‚´ ì²´í¬)
# ==========================================
def is_recent_news(entry):
    if not hasattr(entry, 'published_parsed') or not entry.published_parsed:
        return True
    try:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        current_time = datetime.now(timezone.utc)
        time_diff = current_time - published_time
        
        # 6ì‹œê°„ ê²½ê³¼ ì²´í¬
        if time_diff > timedelta(hours=6):
            print(f"â³ [ì˜¤ë˜ëœ ë‰´ìŠ¤] 6ì‹œê°„ ê²½ê³¼ë¡œ ìŠ¤í‚µ: {time_diff}")
            return False
        return True
    except:
        return True

# ==========================================
# 5. ì´ë¯¸ì§€ ë° AI ê´€ë ¨ í•¨ìˆ˜
# ==========================================
def create_info_image(text_lines, source_name):
    try:
        width, height = 1200, 675 
        background_color = (18, 18, 18)
        text_color = (235, 235, 235)
        title_color = (255, 255, 255)
        accent_color = (0, 190, 255)
        image = Image.new('RGB', (width, height), background_color)
        draw = ImageDraw.Draw(image)
        font_path = "font.ttf"
        try:
            title_font = ImageFont.truetype(font_path, 54) 
            body_font = ImageFont.truetype(font_path, 32)
            source_font = ImageFont.truetype(font_path, 24)
        except: return None

        margin_x = 80       
        header_y = 45
        if source_name: header_text = f"Market Radar | {source_name}"
        else: header_text = "Market Radar"
        draw.text((margin_x, header_y), header_text, font=source_font, fill=accent_color)
        draw.text((margin_x, header_y + 30), "@marketradar0", font=source_font, fill=text_color)

        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST)
        date_str = f"{now.year % 100}ë…„ {now.month}ì›” {now.day}ì¼"
        try: date_width = draw.textlength(date_str, font=source_font)
        except AttributeError: date_width = source_font.getsize(date_str)[0]
        draw.text((width - margin_x - date_width, header_y), date_str, font=source_font, fill=text_color)

        current_y = 140     
        for i, line in enumerate(text_lines):
            line = line.strip().replace("**", "").replace("##", "")
            if not line: continue
            if i == 0: 
                wrapped_lines = textwrap.wrap(line, width=18)
                for wl in wrapped_lines:
                    draw.text((margin_x, current_y), wl, font=title_font, fill=title_color)
                    current_y += 70
                current_y += 25
                draw.line([(margin_x, current_y), (width-margin_x, current_y)], fill=(80, 80, 80), width=2)
                current_y += 45
            else: 
                bullet_size = 10
                bullet_y = current_y + 12
                draw.rectangle([margin_x - 25, bullet_y, margin_x - 25 + bullet_size, bullet_y + bullet_size], fill=accent_color)
                wrapped_lines = textwrap.wrap(line, width=35)
                for wl in wrapped_lines:
                    draw.text((margin_x, current_y), wl, font=body_font, fill=text_color)
                    current_y += 42
                current_y += 10
            if current_y > height - 50: break 
        temp_filename = "temp_card_16_9.png"
        image.save(temp_filename)
        return temp_filename
    except: return None

def extract_image_url(entry):
    if 'media_content' in entry:
        media = entry.media_content[0]
        if 'url' in media: return media['url']
    if 'links' in entry:
        for link in entry.links:
            if link.get('rel') == 'enclosure' and 'image' in link.get('type', ''): return link['href']
    if 'description' in entry:
        urls = re.findall(r'<img[^>]+src="([^">]+)"', entry.description)
        if urls: return urls[0]
    return None

def download_image(url):
    try:
        response = requests.get(url, stream=True, timeout=10)
        if response.status_code == 200:
            filename = "temp_downloaded_image.jpg"
            with open(filename, 'wb') as out_file:
                shutil.copyfileobj(response.raw, out_file)
            return filename
    except: pass
    return None

def get_working_model():
    print("ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ê²€ìƒ‰ ì¤‘...")
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GEMINI_API_KEY}"
    preferred_order = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro"]
    try:
        response = requests.get(url)
        if response.status_code == 200:
            models = [m['name'].replace('models/', '') for m in response.json().get('models', []) if 'generateContent' in m.get('supportedGenerationMethods', [])]
            for pref in preferred_order:
                for model in models:
                    if pref in model: 
                        print(f"âœ… ëª¨ë¸ ì°¾ìŒ: {model}")
                        return model
            if models: return models[0]
    except: pass
    return "gemini-1.5-flash"

def summarize_news(target_model, title, link, content_text=""):
    prompt = f"""
    ë‰´ìŠ¤ ì œëª©: {title}
    ë‰´ìŠ¤ ë§í¬: {link}
    ë‰´ìŠ¤ ë‚´ìš©(Raw): {content_text}
    ë¶„ì„ í›„ íŠ¸ìœ„í„° ë³¸ë¬¸, ì¸í¬ê·¸ë˜í”½ í…ìŠ¤íŠ¸, ì›ì²œ ì†ŒìŠ¤ë¥¼ ì°¾ì•„ì¤˜.
    [ì‘ì„± ê·œì¹™ 1: íŠ¸ìœ„í„° ë³¸ë¬¸]
    - ---BODY--- ì•„ë˜ ì‘ì„±. X í”„ë¦¬ë¯¸ì—„ìš© ì¥ë¬¸ ìƒì„¸ ìš”ì•½. í•œêµ­ì–´ ë²ˆì—­ í•„ìˆ˜. ëª…ì‚¬í˜• ì¢…ê²°/ìŒìŠ´ì²´.
    - êµ¬ì„±: ì œëª©(ì´ëª¨ì§€+í•œê¸€), ìƒì„¸ ë‚´ìš©(âœ… ì²´í¬í¬ì¸íŠ¸), í•˜ë‹¨ í‹°ì»¤($)+í•´ì‹œíƒœê·¸(#)
    [ì‘ì„± ê·œì¹™ 2: ì¸í¬ê·¸ë˜í”½ ì´ë¯¸ì§€]
    - ---IMAGE--- ì•„ë˜ ì‘ì„±.
    - êµ¬ì„±: ì²« ì¤„ ê°•ë ¬í•œ í•œê¸€ ì œëª©(í•µì‹¬ ìˆ˜ì¹˜ í¬í•¨, ì´ëª¨ì§€X). ë‚˜ë¨¸ì§€ í•µì‹¬ ìš”ì•½ 7ë¬¸ì¥ ì´ë‚´.
    [ì‘ì„± ê·œì¹™ 3: ì›ì²œ ì†ŒìŠ¤]
    - ---SOURCE--- ì•„ë˜ ì‘ì„±. ì–¸ë¡ ì‚¬ ì´ë¦„ë§Œ. ì—†ìœ¼ë©´ Unknown.
    [ê¸ˆì§€ì‚¬í•­] ë§ˆí¬ë‹¤ìš´(**, ##) ê¸ˆì§€.
    """
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{target_model}:generateContent?key={GEMINI_API_KEY}"
    data = {"contents": [{"parts": [{"text": prompt}]}], "safetySettings": [{"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"}]}
    headers = {'Content-Type': 'application/json'}
    for _ in range(3):
        try:
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                full_text = response.json()['candidates'][0]['content']['parts'][0]['text']
                if "---BODY---" in full_text and "---IMAGE---" in full_text:
                    parts = full_text.split("---IMAGE---")
                    body_raw = parts[0].replace("---BODY---", "").strip()
                    remaining = parts[1]
                    if "---SOURCE---" in remaining:
                        img_parts = remaining.split("---SOURCE---")
                        image_raw = img_parts[0].strip()
                        source_raw = img_parts[1].strip()
                    else: image_raw = remaining.strip(); source_raw = "Unknown"
                    body_part = body_raw.replace("**", "").replace("##", "")
                    image_lines = [re.sub(r"^[\-\*\â€¢\Â·\âœ…\âœ”\â–ª\â–«\â–º]+\s*", "", re.sub(r"^\d+\.\s+", "", l.strip().replace("**", "").replace("##", ""))) for l in image_raw.split('\n') if l.strip()]
                    source_name = source_raw.split('\n')[0].strip()
                    if "Unknown" in source_name or len(source_name) > 20: source_name = None
                    return body_part, image_lines, source_name
                return None, None, None
            elif response.status_code == 429: time.sleep(60); continue
            else: return None, None, None
        except: return None, None, None
    return None, None, None

# ==========================================
# 6. ê¸°ë¡ ê´€ë¦¬ (ìµœëŒ€ 2000ê°œ ìœ ì§€ & ì¤‘ë³µ ê²€ì‚¬)
# ==========================================
def get_processed_links(filename):
    if not os.path.exists(filename): return []
    # ì½ì–´ì˜¬ ë•Œ ê³µë°± ì œê±°
    with open(filename, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_processed_link(filename, link):
    links = get_processed_links(filename)
    clean_link = link.strip() # â˜… ì €ì¥í•  ë•Œë„ ê³µë°± ì œê±°
    if clean_link not in links:
        links.append(clean_link)
        if len(links) > MAX_HISTORY: links = links[-MAX_HISTORY:]
        with open(filename, 'w', encoding='utf-8') as f: f.write("\n".join(links))

def get_global_titles():
    if not os.path.exists(GLOBAL_TITLE_FILE): return []
    with open(GLOBAL_TITLE_FILE, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_global_title(title):
    titles = get_global_titles()
    clean_title = re.sub(r'\s+', ' ', title).strip()
    if clean_title not in titles:
        titles.append(clean_title)
        if len(titles) > MAX_HISTORY: titles = titles[-MAX_HISTORY:]
        with open(GLOBAL_TITLE_FILE, 'w', encoding='utf-8') as f: f.write("\n".join(titles))

def is_similar_title(new_title, existing_titles):
    clean_new = re.sub(r'\s+', ' ', new_title).strip()
    for old_title in existing_titles:
        if SequenceMatcher(None, clean_new, old_title).ratio() > 0.6: 
            print(f"ğŸš« ì¤‘ë³µ ê°ì§€ (ìœ ì‚¬ë„): {clean_new} <-> {old_title}")
            return True
    return False

# ==========================================
# 7. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================
if __name__ == "__main__":
    current_model = get_working_model()
    global_titles = get_global_titles()
    
    for category, rss_url, filename, default_source_name in RSS_SOURCES:
        print(f"\n--- [{category}] ---")
        
        try:
            feed = feedparser.parse(rss_url)
            if not feed.entries: print("ë‰´ìŠ¤ ì—†ìŒ"); continue
            news = feed.entries[0]
        except: print("RSS íŒŒì‹± ì‹¤íŒ¨"); continue
        
        # 6ì‹œê°„ ì´ë‚´ ì²´í¬
        if not is_recent_news(news):
            continue

        processed_links = get_processed_links(filename)
        # â˜… ë§í¬ ë¹„êµ ì‹œ ê³µë°± ì œê±° í›„ ë¹„êµ (ì•ˆì „ì¥ì¹˜)
        if news.link.strip() in processed_links: 
            print("ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬ (ë™ì¼ URL)"); continue

        check_title = news.title if news.title else (news.description[:50] if hasattr(news, 'description') else "")
        
        # ì¤‘ë³µ ì²´í¬
        if is_similar_title(check_title, global_titles):
            print("íŒ¨ìŠ¤: ë‹¤ë¥¸ ì†ŒìŠ¤ì—ì„œ ì´ë¯¸ ë‹¤ë£¬ ë‚´ìš©."); save_processed_link(filename, news.link); continue

        print(f"âœ¨ ìƒˆ ë‰´ìŠ¤ ë°œê²¬: {news.title}")
        real_link = news.link
        content_for_ai = ""
        if hasattr(news, 'description'):
            content_for_ai = news.description
            if "í…”ë ˆê·¸ë¨" in category:
                urls = re.findall(r'(https?://\S+)', content_for_ai)
                if urls: real_link = urls[0]

        body_text, img_lines, detected_source = summarize_news(current_model, news.title, real_link, content_for_ai)
        
        if body_text and img_lines:
            final_source_name = detected_source if "í…”ë ˆê·¸ë¨" in category else default_source_name
            image_file = create_info_image(img_lines, final_source_name)
            
            try:
                media_id = None
                if image_file: media = api.media_upload(image_file); media_id = media.media_id
                final_tweet = body_text if not final_source_name else f"{body_text}\n\nì¶œì²˜: {final_source_name}"
                if len(final_tweet) > 12000: final_tweet = final_tweet[:11995] + "..."
                if media_id: response = client.create_tweet(text=final_tweet, media_ids=[media_id])
                else: response = client.create_tweet(text=final_tweet)
                tweet_id = response.data['id']
                print("âœ… ì—…ë¡œë“œ ì„±ê³µ")
                client.create_tweet(text=f"ğŸ”— ì›ë¬¸ ê¸°ì‚¬:\n{real_link}", in_reply_to_tweet_id=tweet_id)
                
                # â˜… ì„±ê³µ ì‹œ ë§í¬ ì €ì¥ (ê³µë°± ì œê±°)
                save_processed_link(filename, news.link)
                save_global_title(check_title)
                global_titles.append(re.sub(r'\s+', ' ', check_title).strip())
            except Exception as e: print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")
            if image_file and os.path.exists(image_file): os.remove(image_file)
        else: print("ğŸš¨ ìš”ì•½ ì‹¤íŒ¨")
        time.sleep(2)
