import feedparser
import tweepy
import requests
import os
import sys
import time
import textwrap
import re
import shutil
import json
from difflib import SequenceMatcher
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont
from bs4 import BeautifulSoup

# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ==========================================
def get_clean_env(name):
    val = os.environ.get(name)
    if val is None: return None
    return val.strip().replace('\n', '').replace('\r', '').replace(' ', '')

GEMINI_API_KEY = get_clean_env("GEMINI_API_KEY")
CONSUMER_KEY = get_clean_env("CONSUMER_KEY")
CONSUMER_SECRET = get_clean_env("CONSUMER_SECRET")
ACCESS_TOKEN = get_clean_env("ACCESS_TOKEN")
ACCESS_TOKEN_SECRET = get_clean_env("ACCESS_TOKEN_SECRET")

# ==========================================
# 2. íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
# ==========================================
client = None
api = None
try:
    client = tweepy.Client(
        consumer_key=CONSUMER_KEY,
        consumer_secret=CONSUMER_SECRET,
        access_token=ACCESS_TOKEN,
        access_token_secret=ACCESS_TOKEN_SECRET
    )
    auth = tweepy.OAuth1UserHandler(
        CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET
    )
    api = tweepy.API(auth)
except Exception as e:
    print(f"âš ï¸ íŠ¸ìœ„í„° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")

# ==========================================
# 3. ë‰´ìŠ¤ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
# ==========================================
RSS_SOURCES = [
    ("Investing.com(News)", "https://kr.investing.com/rss/news.rss", "last_link_inv_news.txt", "Investing.com"),
    ("Investing.com(Market)", "https://kr.investing.com/rss/market_overview.rss", "last_link_inv_market.txt", "Investing.com"),
    ("Investing.com(Forex)", "https://kr.investing.com/rss/forex.rss", "last_link_inv_forex.txt", "Investing.com"),
    ("Investing.com(Crypto)", "https://kr.investing.com/rss/290.rss", "last_link_inv_crypto.txt", "Investing.com"),
    ("Investing.com(Economy)", "https://kr.investing.com/rss/286.rss", "last_link_inv_economy.txt", "Investing.com"),
    ("Investing.com(Stock)", "https://kr.investing.com/rss/stock.rss", "last_link_inv_stock.txt", "Investing.com"),
    ("Investing.com(Commodities)", "https://kr.investing.com/rss/commodities.rss", "last_link_inv_comm.txt", "Investing.com"),
    ("Investing.com(Bonds)", "https://kr.investing.com/rss/bonds.rss", "last_link_inv_bonds.txt", "Investing.com"),
    ("íŠ¸ëŸ¼í”„(TruthSocial)", "https://t.me/s/real_DonaldJTrump", "last_id_trump.txt", "Telegram"),
    ("íŠ¸ëŸ¼í”„(Goddess)", "https://t.me/s/goddessTTF", "last_id_goddess.txt", "Telegram"),
    ("í•˜ë‚˜ì°¨ì´ë‚˜(China)", "https://t.me/s/HANAchina", "last_link_hana.txt", "Telegram"),
    ("ë§ˆì´í´ë²„ë¦¬(Burry)", "https://nitter.privacydev.net/michaeljburry/rss", "last_link_burry.txt", "Michael Burry"),
    ("ë¯¸êµ­ì£¼ì‹(ë¸”ë£¸ë²„ê·¸)", "https://news.google.com/rss/search?q=site:bloomberg.com+when:1d&hl=en-US&gl=US&ceid=US:en", "last_link_bloomberg.txt", "Bloomberg"),
    ("ì†ë³´(í…”ë ˆê·¸ë¨)", "https://t.me/s/bornlupin", "last_link_bornlupin.txt", "Telegram"),
    ("ì—°ì˜ˆë‰´ìŠ¤(ì—°í•©)", "https://www.yna.co.kr/rss/entertainment.xml", "last_link_yna_ent.txt", "ì—°í•©ë‰´ìŠ¤"),
    ("êµ­ì œì†ë³´(ì—°í•©)", "https://www.yna.co.kr/rss/international.xml", "last_link_yna_world.txt", "ì—°í•©ë‰´ìŠ¤"),
    ("ì „ìŸì†ë³´(êµ¬ê¸€)", "https://news.google.com/rss/search?q=ì „ìŸ+ì†ë³´+ë¯¸êµ­+ì´ë€&hl=ko&gl=KR&ceid=KR:ko", "last_link_google_war.txt", "Google News"),
    ("ë¯¸êµ­ì£¼ì‹(íˆ¬ì)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069", "last_link_us_investing.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸ˆìœµ)", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664", "last_link_us_finance.txt", "CNBC"),
    ("ë¯¸êµ­ì£¼ì‹(ê¸°ìˆ )", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910", "last_link_us_tech.txt", "CNBC"),
    ("í•œêµ­ì£¼ì‹(í•œê²½)", "https://www.hankyung.com/feed/finance", "last_link_kr.txt", "í•œêµ­ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(Yahoo)", "https://finance.yahoo.com/news/rssindex", "last_link_yahoo.txt", "Yahoo Finance"),
    ("ë¯¸êµ­ì£¼ì‹(Tech)", "https://techcrunch.com/feed/", "last_link_techcrunch.txt", "TechCrunch"),
    ("í•œêµ­ì£¼ì‹(ë§¤ê²½)", "https://www.mk.co.kr/rss/50200011/", "last_link_mk.txt", "ë§¤ì¼ê²½ì œ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Opinion)", "https://feeds.content.dowjones.io/public/rss/RSSOpinion", "last_link_wsj_op.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Market)", "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain", "last_link_wsj_mkt.txt", "WSJ"),
    ("ë¯¸êµ­ì£¼ì‹(WSJ_Economy)", "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed", "last_link_wsj_eco.txt", "WSJ"),
    ("í•œêµ­ì£¼ì‹(ì—°í•©)", "https://www.yna.co.kr/rss/economy.xml", "last_link_yna.txt", "ì—°í•©ë‰´ìŠ¤")
]

MAX_HISTORY = 2000
GLOBAL_TITLE_FILE = "processed_global_titles.txt"
GLOBAL_SUMMARY_FILE = "processed_ai_summaries.txt"

SOURCE_MAP_KR = {
    "Investing.com": "ì¸ë² ìŠ¤íŒ…ë‹·ì»´",
    "Bloomberg": "ë¸”ë£¸ë²„ê·¸",
    "WSJ": "WSJ",
    "CNBC": "CNBC",
    "Yahoo Finance": "ì•¼í›„íŒŒì´ë‚¸ìŠ¤",
    "TechCrunch": "í…Œí¬í¬ëŸ°ì¹˜",
    "Google News": "êµ¬ê¸€ë‰´ìŠ¤",
    "Michael Burry": "ë§ˆì´í´ë²„ë¦¬",
    "ì—°í•©ë‰´ìŠ¤": "ì—°í•©ë‰´ìŠ¤",
    "í•œêµ­ê²½ì œ": "í•œêµ­ê²½ì œ",
    "ë§¤ì¼ê²½ì œ": "ë§¤ì¼ê²½ì œ"
}

# ==========================================
# 4. í¬ë¡¤ë§ ë° ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# ==========================================
class SimpleNews:
    def __init__(self, title, link, description, source_name, filename, published_parsed=None):
        self.title = title
        self.link = link
        self.description = description
        self.source_name = source_name
        self.filename = filename 
        self.published_parsed = published_parsed

def is_recent_news(entry):
    if not hasattr(entry, 'published_parsed') or not entry.published_parsed: return True
    try:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        current_time = datetime.now(timezone.utc)
        time_diff = current_time - published_time
        if time_diff > timedelta(hours=12): 
            return False
        return True
    except: return True

def fetch_telegram_latest(url, source_name, filename):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        messages = soup.select('.tgme_widget_message_wrap')
        if not messages: return None
        
        last_msg = messages[-1]
        text_elem = last_msg.select_one('.tgme_widget_message_text')
        full_text = ""
        if text_elem: full_text = text_elem.get_text(separator="\n").strip()
        
        link_elem = last_msg.select_one('a.tgme_widget_message_date')
        post_link = link_elem['href'] if link_elem else url
        title = full_text.split('\n')[0] if full_text else "í…”ë ˆê·¸ë¨ í¬ìŠ¤íŠ¸"
        if len(title) > 80: title = title[:80] + "..."
        
        return SimpleNews(title, post_link, full_text, source_name, filename)
    except: return None

def fetch_article_content(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        for script in soup(["script", "style", "header", "footer", "nav", "aside", "form"]):
            script.decompose()
        paragraphs = soup.find_all('p')
        article_text = " ".join([p.get_text().strip() for p in paragraphs if len(p.get_text()) > 20])
        if len(article_text) < 100: article_text = soup.get_text(separator=' ', strip=True)
        return article_text[:4000]
    except: return None

# ==========================================
# 5. ì´ë¯¸ì§€ ìƒì„± (ìš”ì•½ ì¹´ë“œ)
# ==========================================
def create_gradient_background(width, height, start_color, end_color):
    base = Image.new('RGB', (width, height), start_color)
    top = Image.new('RGB', (width, height), end_color)
    mask = Image.new('L', (width, height))
    mask_data = []
    for y in range(height): mask_data.extend([int(255 * (y / height))] * width)
    mask.putdata(mask_data)
    base.paste(top, (0, 0), mask)
    return base

def create_info_image(text_lines, source_name, index):
    try:
        width, height = 1200, 675
        bg_start = (10, 25, 45); bg_end = (20, 40, 70)
        text_white = (245, 245, 250); text_gray = (180, 190, 210)
        accent_cyan = (0, 220, 255); title_box_bg = (0, 0, 0, 80)
        image = create_gradient_background(width, height, bg_start, bg_end)
        draw = ImageDraw.Draw(image, 'RGBA')
        try:
            font_title_main = ImageFont.truetype("font_bold.ttf", 55)
            font_body = ImageFont.truetype("font_reg.ttf", 32)
            font_header = ImageFont.truetype("font_bold.ttf", 26)
            font_date = ImageFont.truetype("font_reg.ttf", 26)
        except:
            try:
                font_title_main = ImageFont.truetype("font.ttf", 55)
                font_body = ImageFont.truetype("font.ttf", 32)
                font_header = ImageFont.truetype("font.ttf", 26)
                font_date = ImageFont.truetype("font.ttf", 26)
            except: return None
            
        margin_x = 60; current_y = 40
        
        header_text = f"Koob | News {index}"; 
        if source_name and source_name != "Telegram": header_text += f" | {source_name}"
            
        draw.ellipse([(margin_x, current_y+8), (margin_x+12, current_y+20)], fill=accent_cyan)
        draw.text((margin_x + 25, current_y), header_text, font=font_header, fill=accent_cyan)
        
        KST = timezone(timedelta(hours=9))
        now = datetime.now(KST)
        date_str = f"{now.year}.{now.month:02d}.{now.day:02d} | @kimyg002"
        date_bbox = draw.textbbox((0, 0), date_str, font=font_date)
        date_width = date_bbox[2] - date_bbox[0]
        draw.text((width - margin_x - date_width, current_y), date_str, font=font_date, fill=text_gray)
        current_y += 70
        
        for i, line in enumerate(text_lines):
            clean_line = re.sub(r"^[\W_]+", "", line.strip()) 
            clean_line = clean_line.replace("**", "").replace("##", "")
            if not clean_line: continue
            
            if i == 0: 
                wrapped_title = textwrap.wrap(clean_line, width=22)
                title_box_height = len(wrapped_title) * 80 + 30
                draw.rectangle([(margin_x - 20, current_y), (width - margin_x + 20, current_y + title_box_height)], fill=title_box_bg)
                current_y += 20
                for wl in wrapped_title:
                    draw.text((margin_x, current_y), wl, font=font_title_main, fill=text_white)
                    current_y += 80
                current_y += 40
            else: 
                bullet_y = current_y + 12
                draw.rectangle([margin_x, bullet_y, margin_x + 10, bullet_y + 10], fill=accent_cyan)
                wrapped_body = textwrap.wrap(clean_line, width=42)
                for wl in wrapped_body:
                    draw.text((margin_x + 35, current_y), wl, font=font_body, fill=text_white)
                    current_y += 45
                current_y += 15
            if current_y > height - 60: break 
            
        draw.rectangle([(margin_x, height - 20), (width - margin_x, height - 18)], fill=accent_cyan)
        temp_filename = f"temp_card_{index}.png"
        image.convert("RGB").save(temp_filename)
        return temp_filename
    except Exception as e: 
        print(f"ì´ë¯¸ì§€ ìƒì„± ì—ëŸ¬: {e}")
        return None

# ==========================================
# 6. AI ëª¨ë¸ ë° ì²˜ë¦¬
# ==========================================
def get_working_model():
    print("ğŸ¤– AI ëª¨ë¸ ì¡°íšŒ ì¤‘...")
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GEMINI_API_KEY}"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            models = [m['name'].replace('models/', '') for m in data.get('models', [])]
            priorities = ["gemini-1.5-pro", "gemini-1.5-pro-latest", "gemini-1.5-pro-001", "gemini-pro"]
            for p in priorities:
                if p in models: return p
            return models[0]
    except: pass
    return "gemini-pro"

def select_top_news(news_list, model_name):
    if len(news_list) <= 4: return news_list
    print(f"ğŸ“Š {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ì¤‘ Top 4 ì„ ë³„ ì¤‘...")
    titles = [f"{i}. {n.title} (Source: {n.source_name})" for i, n in enumerate(news_list)]
    titles_text = "\n".join(titles)
    
    prompt = f"""
    You are a professional financial editor.
    Select the **Top 4 most important news items** impacting the global market.
    [News List]
    {titles_text}
    [Output]
    JSON array of indices (e.g. [0, 2, 5, 8])
    """
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={GEMINI_API_KEY}"
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    try:
        response = requests.post(url, headers={'Content-Type': 'application/json'}, json=data)
        text = response.json()['candidates'][0]['content']['parts'][0]['text']
        match = re.search(r'\[.*\]', text, re.DOTALL)
        if match:
            indices = json.loads(match.group())
            selected = [news_list[i] for i in indices if i < len(news_list)]
            return selected[:4]
    except: pass
    return news_list[:4]

def summarize_news_item(target_model, news_item):
    content_text = news_item.description
    if not content_text or len(content_text) < 50:
         fetched = fetch_article_content(news_item.link)
         if fetched: content_text = fetched

    # â˜… [í•µì‹¬ ìˆ˜ì •] í”„ë¡¬í”„íŠ¸: ìì„¸í•œ ë³¸ë¬¸ ë‚´ìš© ìƒì„± ìš”ì²­
    prompt = f"""
    [Task]
    Analyze the provided news and generate outputs.
    
    [Input]
    Title: {news_item.title}
    Source: {news_item.source_name}
    Content: {content_text[:4000]}
    
    [Rules]
    1. Language: **Korean ONLY** for summary.
    2. Terminology: Never use 'ì „ê¸°ë™', always use 'êµ¬ë¦¬'.
    3. Tone: **Abbreviated style (e.g., ~í•¨, ~ìŒ, ~ì „ë§)**. 
    4. **Detail Level for TEXT:**
       - **Do NOT summarize in 1 line.** - Explain the 'Background/Context', 'Key Facts/Numbers', and 'Market Impact' in depth.
       - Each bullet point in the TEXT section must contain **2-3 detailed sentences**.
       - Make it look like a professional analyst's briefing.
    5. **Forbidden:**
       - Do NOT use labels like 'Detailed Point', 'Background:', etc. Just output the content.
       - Do NOT use markdown bold syntax (**text**) in the TEXT section.
    6. **Ticker Extraction:**
       - Identify specific companies or assets mentioned.
       - Convert to Stock Ticker format (e.g., Apple -> $AAPL, Bitcoin -> $BTC).
    
    [Output Format]
    ---IMAGE---
    (Title for Image - 1 line)
    (Short Summary 1 - 1 line)
    (Short Summary 2 - 1 line)
    (Short Summary 3 - 1 line)
    
    ---TEXT---
    (Title for Text - 1 line)
    (Deep Analysis 1: Context/Background - 2~3 sentences ending in noun form)
    (Deep Analysis 2: Key Facts/Numbers - 2~3 sentences ending in noun form)
    (Deep Analysis 3: Market Impact/Outlook - 2~3 sentences ending in noun form)
    (Related Sectors/Assets - 1 line)

    ---TICKERS---
    (Space-separated tickers starting with $, e.g. $AAPL $TSLA $005930. If none, leave empty)
    """
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{target_model}:generateContent?key={GEMINI_API_KEY}"
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        response = requests.post(url, headers={'Content-Type': 'application/json'}, json=data)
        text = response.json()['candidates'][0]['content']['parts'][0]['text']
        
        result_data = {"image": [], "text": [], "tickers": []}
        
        if "---IMAGE---" in text:
            parts_img = text.split("---IMAGE---")[1].split("---TEXT---")
            image_str = parts_img[0].strip()
            remaining = parts_img[1].strip() if len(parts_img) > 1 else ""
            
            if "---TICKERS---" in remaining:
                parts_ticker = remaining.split("---TICKERS---")
                text_str = parts_ticker[0].strip()
                ticker_str = parts_ticker[1].strip()
                
                found_tickers = [t.strip() for t in ticker_str.split() if t.startswith('$')]
                result_data["tickers"] = found_tickers
            else:
                text_str = remaining
            
            result_data["image"] = [l.strip() for l in image_str.split('\n') if l.strip()]
            result_data["text"] = [l.strip() for l in text_str.split('\n') if l.strip()]
            return result_data
        else:
            lines = [l.strip() for l in text.split('\n') if l.strip()]
            return {"image": lines[:4], "text": lines, "tickers": []}
            
    except: return None

# ==========================================
# 7. ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ì¼ê´„ ì²˜ë¦¬ ëª¨ë“œ)
# ==========================================
def get_file_lines(filename):
    if not os.path.exists(filename): return []
    with open(filename, 'r', encoding='utf-8') as f: return [line.strip() for line in f.readlines()]

def save_file_line(filename, line):
    lines = get_file_lines(filename)
    clean_line = re.sub(r'\s+', ' ', line).strip()
    if clean_line not in lines:
        lines.append(clean_line)
        if len(lines) > MAX_HISTORY: lines = lines[-MAX_HISTORY:]
        with open(filename, 'w', encoding='utf-8') as f: f.write("\n".join(lines))

def normalize_text(text):
    text = re.sub(r'[^\w\s]', '', text.lower())
    return set(text.split())

def is_duplicate(new_text, history_lines):
    if not new_text or len(new_text) < 5: return False
    new_words = normalize_text(new_text)
    if len(new_words) < 2: return False
    for old_text in reversed(history_lines):
        old_words = normalize_text(old_text)
        if not old_words: continue
        intersection = len(new_words & old_words)
        union = len(new_words | old_words)
        if union > 0 and (intersection / union) > 0.4: return True
        if SequenceMatcher(None, new_text, old_text).ratio() > 0.55: return True
    return False

if __name__ == "__main__":
    current_model = get_working_model()
    global_titles = get_file_lines(GLOBAL_TITLE_FILE)
    global_summaries = get_file_lines(GLOBAL_SUMMARY_FILE) 
    
    candidates = []
    print("ğŸŒ ì „ì²´ ë‰´ìŠ¤ ì†ŒìŠ¤ ìŠ¤ìº” ì‹œì‘...")
    
    for category, rss_url, filename, source_name in RSS_SOURCES:
        news = None
        if "t.me/s/" in rss_url:
            news = fetch_telegram_latest(rss_url, source_name, filename)
        else:
            try:
                feed = feedparser.parse(rss_url)
                if feed.entries:
                    entry = feed.entries[0]
                    if is_recent_news(entry):
                        news = SimpleNews(entry.title, entry.link, getattr(entry, 'description', ''), source_name, filename)
            except: pass

        if not news: continue
        processed_links = get_file_lines(filename)
        if news.link.strip() in processed_links: continue
        check_content = news.title if news.title else news.description[:100]
        if is_duplicate(check_content, global_titles): continue
        candidates.append(news)

    print(f"âœ… ìˆ˜ì§‘ëœ í›„ë³´ ë‰´ìŠ¤: {len(candidates)}ê°œ")
    if not candidates:
        print("ğŸ“­ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    selected_news = select_top_news(candidates, current_model)
    print(f"ğŸ¯ ìµœì¢… ì„ ë³„ëœ ë‰´ìŠ¤: {len(selected_news)}ê°œ")

    media_ids = []
    
    KST = timezone(timedelta(hours=9))
    now = datetime.now(KST)
    weekday_kor = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][now.weekday()]
    time_str = now.strftime(f"%mì›” %dì¼ ({weekday_kor}) %H:%M")
    
    tweet_text_body = f"ğŸ“… {time_str} ê¸°ì¤€ | ì£¼ìš” ì†Œì‹ ì •ë¦¬\n\n"
    emojis = ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£"]
    
    collected_tickers = set()
    collected_sources = set()

    processed_count = 0
    for i, news in enumerate(selected_news):
        print(f"Processing {i+1}/{len(selected_news)}: {news.title[:20]}...")
        
        if news.source_name != "Telegram":
            safe_source_name = SOURCE_MAP_KR.get(news.source_name, news.source_name)
            collected_sources.add(safe_source_name)

        result = summarize_news_item(current_model, news)
        if not result or not result.get("image"): continue
        
        image_lines = result["image"]
        text_lines = result["text"]
        
        if result.get("tickers"):
            for t in result["tickers"]:
                collected_tickers.add(t)

        image_lines = [l.replace("ì „ê¸°ë™", "êµ¬ë¦¬") for l in image_lines]
        text_lines = [l.replace("ì „ê¸°ë™", "êµ¬ë¦¬") for l in text_lines]
        
        # ë³¼ë“œì²´ ì œê±° ë“± ì²­ì†Œ
        text_lines = [l.replace("**", "").replace("##", "") for l in text_lines]

        joined_summary = " ".join(text_lines)
        if is_duplicate(joined_summary, global_summaries):
            print("  ğŸš« ìš”ì•½ ë‚´ìš© ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ")
            save_file_line(news.filename, news.link)
            continue
            
        img_path = create_info_image(image_lines, news.source_name, i+1)
        if img_path:
            try:
                media = api.media_upload(img_path)
                media_ids.append(media.media_id)
                
                tweet_text_body += f"{emojis[i]} {text_lines[0]}\n" # ì œëª©
                for line in text_lines[1:]:
                    tweet_text_body += f"  â€¢ {line}\n" # ë‚´ìš© (ì´ì œ ê¸¸ê²Œ ë‚˜ì˜´)
                tweet_text_body += "\n" 
                
                save_file_line(news.filename, news.link)
                save_file_line(GLOBAL_TITLE_FILE, news.title if news.title else news.description[:50])
                with open(GLOBAL_SUMMARY_FILE, 'a', encoding='utf-8') as f: f.write(joined_summary + "\n")
                
                processed_count += 1
                if os.path.exists(img_path): os.remove(img_path)
            except Exception as e:
                print(f"  âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

    if media_ids:
        if collected_sources:
            source_str = ", ".join(sorted(list(collected_sources)))
            tweet_text_body += f"ì¶œì²˜ : {source_str}\n"

        base_tags = "#ë¯¸êµ­ì£¼ì‹ #ì†ë³´ #ê²½ì œ"
        ticker_tags = " ".join(list(collected_tickers)) 
        
        tweet_text_body += f"\n{base_tags} {ticker_tags}"
        
        if len(tweet_text_body) > 24000: tweet_text_body = tweet_text_body[:23995] + "..."
        
        try:
            response = client.create_tweet(text=tweet_text_body, media_ids=media_ids)
            print("ğŸš€ [ì„±ê³µ] ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ [ì‹¤íŒ¨] íŠ¸ìœ— ì „ì†¡ ì—ëŸ¬: {e}")
    else:
        print("ğŸ¤· ê²Œì‹œí•  ìœ íš¨í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
